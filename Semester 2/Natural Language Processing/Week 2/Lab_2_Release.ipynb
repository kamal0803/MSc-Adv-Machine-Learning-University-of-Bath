{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Sentiment Analysis\n",
        "*   **Course**: CM52065 Natural Language Processing\n",
        "*   **Author**: Dr. Andrew Barnes\n",
        "\n",
        "## Overview\n",
        "This lab will provide a chance for you to implement some of the representations we saw in this week's lectures.\n",
        "\n",
        "The learning objectives for this lab are as follows:\n",
        "\n",
        "1. Implement BoNGrams document representation using Python.\n",
        "2. Implement TF-IDF transformation.\n",
        "3. Apply document representations to a sentiment analysis use-case.\n",
        "\n",
        "To achieve these we will be using a sentiment analysis use-case.\n",
        "\n",
        "Sentiment analysis is the method of estimating the sentiment of a given document. For example, 'The latest innovations from Company X were terrible.' has a strongly negative sentiment; whereas, 'The employees were thrilled at the increase in wages.' has a positive sentiment."
      ],
      "metadata": {
        "id": "KKm33CdWNcUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset we will be working with this week is a set of IMDB movie reviews and the review was positive or negative.\n",
        "\n",
        "### 1.1 Loading the dataset\n",
        "You will need to make sure the `Lab 2 Dataset.tar.gz` file is in the same folder as this script and has been extracted into a folder called `Lab 2 Dataset`. I have provided a method below to load the data into both train and test for you.\n",
        "\n",
        "If you are using the Google Colab interface you will need to upload the `.tar.gz` file and then extract it by running the commented out code below."
      ],
      "metadata": {
        "id": "ZvO3cnO5kjAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj8BnaRaK1af",
        "outputId": "fcb4b210-27e2-4219-f6b8-40e18c12ef2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/drive/MyDrive/MSc Advanced Machine Learning/Semester 2/Natural Language Processing/Week 2/Lab 2 Dataset.gz'"
      ],
      "metadata": {
        "id": "96f5CbdaK8yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ONLY RUN THIS CODE BLOCK IF YOU NEED TO EXTRACT A .tar.gz FILE ON GOOGLE COLAB\n",
        "import tarfile\n",
        "tar = tarfile.open(file_name, \"r\")\n",
        "#tar = tarfile.open('Lab 2 Dataset.gz', \"r\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sex7k7M8bCIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bbf905-e9b4-4b32-a0d5-f41a510fe794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3260246467.py:5: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_dataset(split='train'):\n",
        "  data = []\n",
        "  sentiment = {\"pos\": 1, \"neg\": 0}\n",
        "  split_path = os.path.join('./aclImdb/', split)\n",
        "  for label in [\"pos\", \"neg\"]:\n",
        "      label_path = os.path.join(split_path, label)\n",
        "      for fname in os.listdir(label_path):\n",
        "          if fname.endswith(\".txt\"):\n",
        "              with open(os.path.join(label_path, fname), encoding=\"utf-8\") as f:\n",
        "                  text = f.read().strip()\n",
        "                  data.append([sentiment[label], text])\n",
        "  return pd.DataFrame(data, columns=['label', 'text']).sample(500)\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "dataset = load_dataset(split=\"train\")\n",
        "print(f\"Loaded {len(dataset)} examples.\")\n"
      ],
      "metadata": {
        "id": "h_VmjzkRRfEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa97738-0b94-4935-eab8-719e40a7b197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loaded 500 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a couple examples:"
      ],
      "metadata": {
        "id": "viiaxus5UNBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example of a positive review:\")\n",
        "print(dataset[dataset['label'] == 1].iloc[50]['text'])\n",
        "print(\"Example of a negative review:\")\n",
        "print(dataset[dataset['label'] == 0].iloc[50]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVpErlOOUPJV",
        "outputId": "23e36e88-c5c4-42f1-dbea-b9c808990497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of a positive review:\n",
            "Someone(or, something thing..)is leaving puncture marks on the jugular and draining victims of their blood till dead. Police detective Karl Brettschneider(Melvyn Douglas, before slipping out of the B-movie horror genre for greater heights)is stumped at who..or what..is behind these notorious crimes. The village is overcome by hysteria and Karl depends on his trusted medical genius, Dr. Otto von Niemann(Lionel Atwill, in yet another effective mad scientist role)to provide some feedback as to what might be causing the deaths of innocents. He also fears for the safety of his beloved Ruth(the lovely Fay Wray who stars for the third time with Atwill after \"Doctor X\" & \"The Mystery of the Wax Museum\")who is Niemann's assistant.<br /><br />Dwight Frye steals the film as a rather loony village idiot who collects bats and carries a demented demeanor wherever he goes..it's easy to see why he becomes a suspect as local paranoia is at a fever pitch. Maude Eburne provides the film's humor as a very naive(..and easily influenced)patient of von Niemann's who believes she has ailments she reads about in books near the laboratory where he works. She's impressionable and often von Niemann just humors her and constant fictional illnesses she feels plagued with. Lionel Belmore returns as yet another frightened, superstitious BÃ¼rgermeister.<br /><br />Creaky, static, but rather entertaining nonetheless thanks to the cast. The film is obviously as low-budget as they come, but this doesn't hurt the film too much since it's put together rather well by director Frank R Strayer and his crew. I'm certain the film's print has seen better days, though. This is the kind of B-horror item you'd find packaged in with 50 other random cheesefests and poverty row programmers. The film's villain..and his motives for feeding a synthetically made biological creature..certainly provides a different take on the Frankenstein formula. Many might be disappointed with the end results as the film strays away from being an actual supernatural tale about a real vampire killer causing the murders.\n",
            "Example of a negative review:\n",
            "This is by far the worst movie I have ever seen in the cinema!! Could not wait for it to end. To make matters worse it is given a 12A certificate so you do not see anyone getting shot, just bodies slumping to the ground, even Babban getting killed was cut out!!! Too many scenes were cut to bring in the younger viewers as I think the makers knew it would flop disastrously!! Amitabhs acting was great but that 'Basanti' wannabe and the other idiot who plays Devgans mate can't even act. Devgan was wasted!!<br /><br />I would not watch this for free again and I advise all others who read this to do just the same YOU HAVE BEEN WARNED!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Representing the Corpus\n",
        "\n",
        "The first step to achieving our goal of building a sentiment classifier is to build our document representations using the techniques we learned in the lectures."
      ],
      "metadata": {
        "id": "CjdWvENFi8Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words Representations\n",
        "\n",
        "In this section you will implement the three types of representation we learned in the lectures this week. Don't worry though, we may start out from first principles but we will move to libraries soon!"
      ],
      "metadata": {
        "id": "Fk-K0uhNf2sH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag of Unigrams\n",
        "Your first task is to implement a method which will return a Bag of Unigrams representation for a given set of documents. Remember, you will first need to generate a vocabulary, and then build the feature set."
      ],
      "metadata": {
        "id": "UCZDGQkkgGp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_unigrams(documents):\n",
        "  vocabulary = {}\n",
        "  dataset = []\n",
        "  #### BEGIN CODE ####\n",
        "  for document in documents:\n",
        "    for word in document:\n",
        "      if word not in vocabulary:\n",
        "        vocabulary[word] = len(vocabulary)\n",
        "\n",
        "\n",
        "  for document in documents:\n",
        "    ds = [0]*len(vocabulary)\n",
        "    for word in document:\n",
        "      if word in vocabulary:\n",
        "        x = vocabulary[word]\n",
        "        ds[x] = 1\n",
        "    dataset.append(ds)\n",
        "\n",
        "\n",
        "  #### END CODE ####\n",
        "  return dataset, vocabulary"
      ],
      "metadata": {
        "id": "kFeMhDu7ihCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try your method out with the test function below!"
      ],
      "metadata": {
        "id": "ngB-KSjVeNuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_unigram(method):\n",
        "  test, testv = method([['i', 'love', 'dogs'], ['i', 'cats']])\n",
        "  if sum(test[0]) == 3:\n",
        "      print(\"Test doc #1 passed.\")\n",
        "  else:\n",
        "      print(f\"Test doc #1 failed. Expected something like [1, 1, 1, 0, 0], got {test[0]}\")\n",
        "  if sum(test[1]) == 2:\n",
        "      print(\"Test doc #2 passed.\")\n",
        "  else:\n",
        "      print(f\"Test doc #2 failed. Expected something like [1, 0, 0, 1, 1], got {test[1]}\")\n",
        "test_unigram(bag_of_unigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7tRP-LpeILM",
        "outputId": "edaffd20-30c8-4a5d-e5b9-4f91149b93a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test doc #1 passed.\n",
            "Test doc #2 passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag of N-Grams\n",
        "\n",
        "Now it's time to expand this function to implement Bag of N-grams where N can be any integer > 1 (but within a reasonable range, there is no point having an N which is the same size as a document (unless you're looking for an exact match)).\n",
        "\n",
        "The process is very similar to your previous method but this time you need to be more expansive with your dictionary.\n",
        "\n",
        "HINT: Using nltk's `defaultdict` will save you some code."
      ],
      "metadata": {
        "id": "fAmavYtugKuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import defaultdict\n",
        "def bag_of_ngrams(documents, n=1):\n",
        "  vocabulary = {}\n",
        "  dataset = []\n",
        "  ### START CODE HERE ###\n",
        "  for document in documents:\n",
        "    for i in range(len(document)):\n",
        "        if ' '.join(document[i:i+n]) not in vocabulary and len(document[i:i+n]) == n:\n",
        "            vocabulary[' '.join(document[i:i+n])]= len(vocabulary)\n",
        "\n",
        "\n",
        "  for document in documents:\n",
        "      ds = [0]*len(vocabulary)\n",
        "      for i in range(len(document)):\n",
        "          if len(document[i:i+n]) == n and ' '.join(document[i:i+n]) in vocabulary:\n",
        "              x = vocabulary[' '.join(document[i:i+n])]\n",
        "              ds[x] = 1\n",
        "      dataset.append(ds)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "  return dataset, vocabulary"
      ],
      "metadata": {
        "id": "E8B8r7RbfX_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to test your method:"
      ],
      "metadata": {
        "id": "Zs4zJPjXkZ6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_bigram(method):\n",
        "  test, testv = method([['i', 'love', 'dogs'], ['like', 'cats']], 2)\n",
        "  if sum(test[0]) == 2:\n",
        "      print(\"Test doc #1 passed.\")\n",
        "  else:\n",
        "      print(f\"Test doc #1 failed. Expected something like [1, 1, 0], got {test[0]}\")\n",
        "  if sum(test[1]) == 1:\n",
        "      print(\"Test doc #2 passed.\")\n",
        "  else:\n",
        "      print(f\"Test doc #2 failed. Expected something like [0, 0, 1], got {test[1]}\")\n",
        "def test_trigram(method):\n",
        "  test, testv = method([['i', 'love', 'dogs', 'sometimes'], ['sometimes', 'like', 'cats']], 3)\n",
        "  if sum(test[0]) == 2:\n",
        "      print(\"Test doc #1 passed.\")\n",
        "  else:\n",
        "      print(f\"Test doc #1 failed. Expected something like [1, 1, 0], got {test[0]}\")\n",
        "  if sum(test[1]) == 1:\n",
        "      print(\"Test doc #2 passed.\")\n",
        "  else:\n",
        "      print(f\"Test doc #2 failed. Expected something like [0, 0, 1], got {test[1]}\")\n",
        "print(\"Testing Unigram\")\n",
        "test_unigram(bag_of_ngrams)\n",
        "print(\"Testing Bigram\")\n",
        "test_bigram(bag_of_ngrams)\n",
        "print(\"Testing Trigram\")\n",
        "test_trigram(bag_of_ngrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSvd4aFHfX8a",
        "outputId": "f7b7c5a9-a76d-4c3d-ee8a-50b2fd6027b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Unigram\n",
            "Test doc #1 passed.\n",
            "Test doc #2 passed.\n",
            "Testing Bigram\n",
            "Test doc #1 passed.\n",
            "Test doc #2 passed.\n",
            "Testing Trigram\n",
            "Test doc #1 passed.\n",
            "Test doc #2 passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF\n",
        "\n",
        "It's now time to implement the TF-IDF method. However, because the TF-IDF representation is a modification to the Bag of N-Grams method we can use this to our advantage.\n",
        "\n",
        "Complete the method below to implement TF-IDF on a given BoNGrams representation."
      ],
      "metadata": {
        "id": "boLCxaaRoOAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def tfidf_transformation(feature_vectors):\n",
        "  transformed_feature_vectors = []\n",
        "  # Let's transform the feature vectors into a matrix\n",
        "  feature_vectors = np.array(feature_vectors)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  tf = feature_vectors / feature_vectors.sum(axis=1, keepdims=True)\n",
        "\n",
        "  N = feature_vectors.shape[0]\n",
        "  idf = np.zeros(feature_vectors.shape[1])\n",
        "\n",
        "  for j in range(feature_vectors.shape[1]):  # loop over columns\n",
        "    df = np.sum(feature_vectors[:, j] > 0)\n",
        "    idf[j] = np.log(N / df)\n",
        "\n",
        "\n",
        "  transformed_feature_vectors = (tf * idf).tolist()\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return transformed_feature_vectors\n"
      ],
      "metadata": {
        "id": "42MPqZS4p30d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your code below, you should see the following outputs (depending on how you structured your vocabulary):\n",
        "\n",
        "1. `[[0, 0.23., 0.46., 0], [0, 0, 0, 2.38.]]`\n",
        "2. `[[0.69., 0], [0, 0.69.]]`\n",
        "3. `[[0.69., 0], [0, 0]]`\n"
      ],
      "metadata": {
        "id": "TknsPyWML5zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_transformation([[1, 1, 2, 0], [1, 0, 0, 4]]))\n",
        "print(tfidf_transformation([[1, 0], [0, 1]]))\n",
        "print(tfidf_transformation([[1, 1], [0, 500]]))"
      ],
      "metadata": {
        "id": "7O-CZ55bfXsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a32350-430c-46a2-8177-15abfe687835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.17328679513998632, 0.34657359027997264, 0.0], [0.0, 0.0, 0.0, 0.5545177444479562]]\n",
            "[[0.6931471805599453, 0.0], [0.0, 0.6931471805599453]]\n",
            "[[0.34657359027997264, 0.0], [0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Wrangling\n",
        "\n",
        "We now have our data encoding methods in place so now it's time to wrangle our data and transform it into something useable.\n",
        "\n",
        "To do this we will be making use of a new library called spaCy which is another popular NLP library. spaCy offers a range of preprocessing and tokenisation options for our corpus, take some time to explore spaCy's capabilities [here](https://www.geeksforgeeks.org/nlp/tokenization-using-spacy-library/). Whilst it may seem insignificant now this library will become very useful later on.\n",
        "\n",
        "To save time you have been provided a simple tokenisation example below which is all you need for this lab, please do modify the example sentences to get a feel for what is happening.\n",
        "\n",
        "NOTE: You may notice spacy considers case of letters (e.g. the vs The) and punctuation seperately. For our purposes today this is fine, but consider how you could preprocess the tweets further."
      ],
      "metadata": {
        "id": "MndTLhD1pR-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "def process_document(corpus):\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  processed_corpus = []\n",
        "  for d in corpus:\n",
        "    doc = nlp(d) # Tokenise\n",
        "    doc_tokens = []\n",
        "    for token in doc:\n",
        "      if not token.is_stop: # Check for stop words\n",
        "        doc_tokens.append(token.text)\n",
        "    processed_corpus.append(doc_tokens)\n",
        "  return processed_corpus"
      ],
      "metadata": {
        "id": "dKd1UPC3fXmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(process_document([\"this is a new document given to spacy.\"]))\n",
        "print(process_document([\"The dog in the hole barked loudly into the wind.\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wweOMBX4PvkU",
        "outputId": "3eb9bd20-751b-49f7-996a-48bc6c996c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['new', 'document', 'given', 'spacy', '.']]\n",
            "[['dog', 'hole', 'barked', 'loudly', 'wind', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's your turn! Expand the template method provided below to process and encode the datasets provided.\n",
        "\n",
        "You will need to use:\n",
        "1. Preprocessing `process_document(...)`\n",
        "2. Bag of N-Grams `bag_of_ngrams(...)`\n",
        "3. TFIDF `tfidf_transformation(...)`"
      ],
      "metadata": {
        "id": "38DG9mkhQZac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_corpus(corpus, n=2, tfidf=False):\n",
        "  \"\"\"\n",
        "  `n`:: The 'n' in the n-Gram calculation.\n",
        "  `tfidf`:: Whether to apply TF-IDF transformation.\n",
        "  \"\"\"\n",
        "  encoding = corpus\n",
        "  ### START CODE ###\n",
        "\n",
        "  encoding = process_document(corpus)\n",
        "  encoding, _ = bag_of_ngrams(encoding, n)\n",
        "  encoding = tfidf_transformation(encoding)\n",
        "  ### END CODE ###\n",
        "  return np.array(encoding)"
      ],
      "metadata": {
        "id": "yhK68FLOQ8v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will use the code above to generate training and test datasets for different scenarios:\n",
        "\n",
        "1. Unigrams without TF-IDF\n",
        "2. Unigrams with TF-IDF\n",
        "3. Trigrams without TF-IDF\n",
        "4. Trigrams with TF-IDF"
      ],
      "metadata": {
        "id": "3y2xcM0cQ9K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Encoding Unigram case...\")\n",
        "ug_raw = encode_corpus(dataset['text'])\n",
        "print(\"Encoding Trigram case...\")\n",
        "tg_raw = encode_corpus(dataset['text'], n=3)\n",
        "print(\"Encoding Unigram TF-IDF case...\")\n",
        "ug_tfidf = encode_corpus(dataset['text'], tfidf=True)\n",
        "print(\"Encoding Trigram TF-IDF case...\")\n",
        "tg_tfidf = encode_corpus(dataset['text'], n=3, tfidf=True)"
      ],
      "metadata": {
        "id": "AZ-EC9l6RJhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0b3c33-ee11-4d10-f7a6-f35b0df5793b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding Unigram case...\n",
            "Encoding Trigram case...\n",
            "Encoding Unigram TF-IDF case...\n",
            "Encoding Trigram TF-IDF case...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this seemed eerily fast that's because we only used a random sample of 250 documents from the training and testing datasets (check out the Dataset loading code to see this). You are more than welcome to increase this, but be warned, the size of your feature vectors is going to get large.\n",
        "\n",
        "For context, I've provided some informative print outs below, check these and how much RAM you have before you increase the size!"
      ],
      "metadata": {
        "id": "z0nZ6jcSujv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def convert_size(size_bytes):\n",
        "   if size_bytes == 0:\n",
        "       return \"0B\"\n",
        "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
        "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
        "   p = math.pow(1024, i)\n",
        "   s = round(size_bytes / p, 2)\n",
        "   return \"%s %s\" % (s, size_name[i])\n",
        "\n",
        "print(f\"Size of the Unigram feature set: {ug_raw.shape[0]}x{ug_raw.shape[1]} which takes up {convert_size(ug_raw.nbytes)}.\")\n",
        "print(f\"Size of the Unigram TF-IDF feature set: {ug_tfidf.shape[0]}x{ug_tfidf.shape[1]} which takes up {convert_size(ug_tfidf.nbytes)}.\")\n",
        "print(f\"Size of the Trigram feature set: {tg_raw.shape[0]}x{tg_raw.shape[1]} which takes up {convert_size(tg_raw.nbytes)}.\")\n",
        "print(f\"Size of the Trigram TF-IDF feature set: {tg_tfidf.shape[0]}x{tg_tfidf.shape[1]} which takes up {convert_size(tg_tfidf.nbytes)}.\")\n"
      ],
      "metadata": {
        "id": "qDDVOhCVRLCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1195d0-b626-4580-c90a-f2bb05316d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the Unigram feature set: 500x52134 which takes up 198.88 MB.\n",
            "Size of the Unigram TF-IDF feature set: 500x52134 which takes up 198.88 MB.\n",
            "Size of the Trigram feature set: 500x64807 which takes up 247.22 MB.\n",
            "Size of the Trigram TF-IDF feature set: 500x64807 which takes up 247.22 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question:** Why does the trigram approach result in a larger vocabulary?\n",
        "\n",
        "> **Question:** How would you expect the vocabulary size to change as we increase `n` to larger values? (e.g. `n=5`, `n=10`)"
      ],
      "metadata": {
        "id": "UZLgP59uTl7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Building a Classifier\n",
        "\n",
        "Now let's build our Naive-Bayes Classifier.\n",
        "\n",
        "Populate the method below to accept a training dataset and output a Naive-Bayes model.\n",
        "\n",
        "Hint: If you're stuck on how to create the classifier, try going [here](https://scikit-learn.org/stable/modules/naive_bayes.html)."
      ],
      "metadata": {
        "id": "rhlzdKFh1n-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "def train_model(encoded_corpus, labels):\n",
        "  model = GaussianNB()\n",
        "  ### START CODE HERE ###\n",
        "\n",
        "  model.fit(encoded_corpus, labels)\n",
        "\n",
        "  ### END CODE HERE\n",
        "  return model"
      ],
      "metadata": {
        "id": "OcnzwGmY1nax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's some code to split the data and create the models using your method above."
      ],
      "metadata": {
        "id": "RbM7C5wO0Som"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data for a unigram approach\n",
        "ug_raw_train, ug_raw_test, ug_raw_labels_train, ug_raw_labels_test = train_test_split(\n",
        "    ug_raw, dataset['label'], test_size=0.33, random_state=42)\n",
        "# Split the data for a unigram approach with TF-IDF\n",
        "ug_tfidf_train, ug_tfidf_test, ug_tfidf_labels_train, ug_tfidf_labels_test = train_test_split(\n",
        "    ug_tfidf, dataset['label'], test_size=0.33, random_state=42)\n",
        "# Split the data for a trigram approach\n",
        "tg_raw_train, tg_raw_test, tg_raw_labels_train, tg_raw_labels_test = train_test_split(\n",
        "    tg_raw, dataset['label'], test_size=0.33, random_state=42)\n",
        "# Split the data for a trigram approach with TF-IDF\n",
        "tg_tfidf_train, tg_tfidf_test, tg_tfidf_labels_train, tg_tfidf_labels_test = train_test_split(\n",
        "    tg_tfidf, dataset['label'], test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "axpb32vvzR-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training unigram model...\")\n",
        "ug_raw_model = train_model(ug_raw_train, ug_raw_labels_train)\n",
        "print(\"Training unigram with TF-IDF model...\")\n",
        "ug_tfidf_model = train_model(ug_tfidf_train, ug_tfidf_labels_train)\n",
        "print(\"Training trigram model...\")\n",
        "tg_raw_model = train_model(tg_raw_train, tg_tfidf_labels_train)\n",
        "print(\"Training trigram with TF-IDF model...\")\n",
        "tg_tfidf_model = train_model(tg_tfidf_train, tg_tfidf_labels_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-a5IRw1z1LR",
        "outputId": "250b382a-de85-464d-a76d-9e221c1094de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training unigram model...\n",
            "Training unigram with TF-IDF model...\n",
            "Training trigram model...\n",
            "Training trigram with TF-IDF model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to test our classifier, using the imported metrics perform an evaluation of the four encoding schemes.\n",
        "\n",
        "As you did this in the last lab, I've included code to evaluate the models."
      ],
      "metadata": {
        "id": "uaL2FTgOxzO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "def evaluate_model(model_name, model, test_features, test_labels):\n",
        "  predictions = model.predict(test_features)\n",
        "  precision = precision_score(test_labels, predictions)\n",
        "  recall = recall_score(test_labels, predictions)\n",
        "  accuracy = accuracy_score(test_labels, predictions)\n",
        "  print(f\"{model_name}: Acc. - {accuracy:.2f}%, Prec. - {precision:.2f}%, Rec. - {recall:.2f}%, \")"
      ],
      "metadata": {
        "id": "Huk4XcLk2z-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(\"Unigram Raw\", ug_raw_model, ug_raw_test, ug_raw_labels_test)\n",
        "evaluate_model(\"Unigram TF-IDF\", ug_tfidf_model, ug_tfidf_test, ug_tfidf_labels_test)\n",
        "evaluate_model(\"Trigram Raw\", tg_raw_model, tg_raw_test, tg_raw_labels_test)\n",
        "evaluate_model(\"Trigram TF-IDF\", tg_tfidf_model, tg_tfidf_test, tg_tfidf_labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdUvTYUe1Xzn",
        "outputId": "744c9cc4-5145-47ff-8303-4a3024873ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Raw: Acc. - 0.67%, Prec. - 0.64%, Rec. - 0.58%, \n",
            "Unigram TF-IDF: Acc. - 0.67%, Prec. - 0.64%, Rec. - 0.58%, \n",
            "Trigram Raw: Acc. - 0.58%, Prec. - 0.52%, Rec. - 0.68%, \n",
            "Trigram TF-IDF: Acc. - 0.58%, Prec. - 0.52%, Rec. - 0.68%, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question:** Can you explain why the models produced different results?\n",
        "\n",
        "> **Question:** What effect does the train, test split have on the results?\n",
        "\n",
        "> **Question:** What changes could you make to further improve the results?"
      ],
      "metadata": {
        "id": "GhZPTCv_2Dsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap-up\n",
        "And that's the end of this week's lab! I hope you enjoyed implementing and using the representations we learned this week.\n",
        "\n",
        "If you fancy an additional challenge, can you implement sparsity reduction techniques and evaluate their ability to improve results?"
      ],
      "metadata": {
        "id": "zeru8w3f6qVQ"
      }
    }
  ]
}