{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a682aefd-8eb7-418c-809b-28808ffc73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b220ba7-822a-44b4-9ec5-ba0f9c7980ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a19ec86b-6902-420e-a0e7-f520968eceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "print(type(data_train.data))\n",
    "print(len(data_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3b1a043-a7a3-4b7f-b6b8-a7a7c9b7d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'\n"
     ]
    }
   ],
   "source": [
    "print(repr(data_train.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3c312f3a-39cc-4703-8bb5-49984f32870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(texts):\n",
    "  return [t.lower() for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e37f713-7f27-4569-8eeb-c057bc97b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def remove_punctuation(texts):\n",
    "\n",
    "  list_punc = list(punctuation)\n",
    "\n",
    "  cleaned_texts = []\n",
    "  for text in texts:\n",
    "    for punc in list_punc:\n",
    "      text = text.replace(punc, \"\")\n",
    "    cleaned_texts.append(text)\n",
    "\n",
    "  \n",
    "  return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "045c02b5-08ee-44c5-b05e-36e426a983b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hidden_characters(texts):\n",
    "  cleaned_texts = []\n",
    "\n",
    "  for text in texts:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    cleaned_texts.append(text)\n",
    "\n",
    "  return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8741b247-a0a8-4a98-b236-5e0e2e5685ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_removal(texts):\n",
    "  return [t.strip() for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ead51871-dd70-40dd-bf40-277783192248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/kamalrajanisrani/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c94dff0-6760-4c53-bdb1-8d8798ccd1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'words', 'are', 'being', 'split', 'up']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"My words are being split up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "370d1018-3a3e-4f68-9b85-b015b5919ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(texts):\n",
    "  tokenised_texts = []\n",
    "  \n",
    "  for text in texts:\n",
    "    tokenised_texts.append(word_tokenize(text))\n",
    "\n",
    "  return tokenised_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f0ca4b7d-e8cf-4d4b-aac2-84341870ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kamalrajanisrani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kamalrajanisrani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b20f13d3-949f-42dc-86e9-631505cda19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    cleaned_texts = []\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    for text in texts:\n",
    "        cleaned_sentences = []\n",
    "        for word in text:\n",
    "            if str(word) not in stop_words:\n",
    "                cleaned_sentences.append(word)\n",
    "        cleaned_texts.append(cleaned_sentences)\n",
    "\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "81a19326-38fa-4a1c-95e7-66b63339b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_texts(texts):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    return [[stemmer.stem(word) for word in text] for text in texts]\n",
    "\n",
    "def lemmatise_texts(texts):\n",
    "    \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    return [[lemmatiser.lemmatize(word) for word in text] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b70a0d6f-1479-451b-b1ed-8b39f64fce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, reduction='s'):\n",
    "  \n",
    "    texts = lowercase(texts)\n",
    "    texts = remove_punctuation(texts)\n",
    "    texts = remove_hidden_characters(texts)\n",
    "    texts = whitespace_removal(texts)\n",
    "    texts = tokenise(texts)\n",
    "    texts = remove_stopwords(texts)\n",
    "    \n",
    "  # Allow a choice as to whether stem or lemmtise\n",
    "    if reduction == 's':\n",
    "        texts = stem_texts(texts)\n",
    "    else:\n",
    "        texts = lemmatise_texts(texts)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ddc9db5-b20a-4845-891d-479394109b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ae478e63-6d51-4650-acd8-4f1fd2310eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def encode(texts, vocabulary=None):\n",
    "    vocabulary = build_vocabulary(texts) if vocabulary is None else vocabulary\n",
    "    dataset = []\n",
    "\n",
    "    for text in texts:\n",
    "        ds = [0] * len(vocabulary)\n",
    "        for word in text:\n",
    "            if word in vocabulary:\n",
    "                ds[vocabulary[word]] = 1\n",
    "                \n",
    "        dataset.append(ds)\n",
    "\n",
    "    return np.array(dataset), vocabulary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9969e951-0341-4adb-99da-d2b9905f2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_vocabulary = encode(preprocess(data_train.data, reduction='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68c4e432-d1b8-4f89-b87f-0119eaf4afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = data_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a970fc8-d184-4d5c-82ab-a3ac3f62cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f8abc8f-ce96-434b-8de8-ce6d07f2d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Retrieve the test data\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "\n",
    "# Encode the test data\n",
    "test_features, _ = encode(preprocess(data_test.data, reduction='s'), vocabulary=train_vocabulary)\n",
    "test_targets = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e881a2b-ad20-48e0-909a-83f00f347363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6197557089750398\n",
      "Precision: 0.6188545610367717\n",
      "Recall: 0.6093566476537501\n"
     ]
    }
   ],
   "source": [
    "# For stemming\n",
    "y_pred = clf.predict(test_features)\n",
    "print(f\"Accuracy: {accuracy_score(test_targets, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(test_targets, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(test_targets, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b7c24-7835-4a82-9519-cb0ce41a7363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "af819887-c56a-45ad-b9e8-070597ebf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_vocabulary = encode(preprocess(data_train.data, reduction='l'), vocabulary=None)\n",
    "test_features, _ = encode(preprocess(data_test.data, reduction='l'), vocabulary=train_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9300f378-2ae8-4ce1-9fb0-f67f6246f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6153744025491238\n",
      "Precision: 0.613371885022185\n",
      "Recall: 0.6044157604291098\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(train_features, train_targets)\n",
    "\n",
    "# For lemmatisation\n",
    "lemma_pred = clf.predict(test_features)\n",
    "print(f\"Accuracy: {accuracy_score(test_targets, lemma_pred)}\")\n",
    "print(f\"Precision: {precision_score(test_targets, lemma_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(test_targets, lemma_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8e8f3-3536-4dec-a2bb-2ddee5387cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
