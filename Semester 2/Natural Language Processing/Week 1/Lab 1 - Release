{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1IC-bu30OF2hTWeGS49rsnB4LTPDeoqPp","timestamp":1770278703671}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab 1: Processing Text\n","*   **Course**: CM52065 Natural Language Processing\n","*   **Author**: Dr. Andrew Barnes\n","\n","## Overview\n","In this lab you will explore the steps in text preprocessing to identify their effects on a text classification task.\n","\n","The learning objectives for this lab are as follows:\n","\n","1. Implement a text processing pipeline.\n","2. Implement a deterministic classification algorithm for NLP.\n","3. Compare and contrast the effects of different preprocessing techniques.\n","\n","To achieve these objectives you will begin by implementing the stages of a text processing pipeline using the popular `nltk` library. You will then use this pipeline on a text classification task.\n","\n","Specifically, you will be creating a categorisation tool for posts on newsgroups."],"metadata":{"id":"KKm33CdWNcUe"}},{"cell_type":"markdown","source":["## Dataset\n","\n","The dataset we will be using contains a set of newsgroup posts on a variety of categories, you can read more about the dataset here: https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset.\n","\n","### 1.1 Loading the dataset\n","This is a very popular dataset in the machine learning world, so we will cheat slightly by using sklearn to handle the import."],"metadata":{"id":"ZvO3cnO5kjAc"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups"],"metadata":{"id":"h_VmjzkRRfEz","executionInfo":{"status":"ok","timestamp":1770400107266,"user_tz":0,"elapsed":1312,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["We will now extract the training data for four categories:\n","+ Motorcycles\n","+ Hockey\n","+ Electronics\n","+ Space\n","\n","This import provides us a useful helping hand to remove headers and footers. We could parse these posts manually and handle the extract detail and tags, but let's not duplicate effort!"],"metadata":{"id":"UQjwKDSChIAo"}},{"cell_type":"code","source":["data_train = fetch_20newsgroups(\n","    subset=\"train\",\n","    categories=[\n","      'rec.motorcycles',\n","      'rec.sport.hockey',\n","      'sci.electronics',\n","      'sci.space'\n","    ],\n","    shuffle=True,\n","    random_state=42,\n","    remove=(\"headers\", \"footers\", \"quotes\"),\n",")"],"metadata":{"id":"3_JgPTazTV54","executionInfo":{"status":"ok","timestamp":1770400109241,"user_tz":0,"elapsed":1974,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Now we have some data, let's take a look at a couple of the posts:"],"metadata":{"id":"B9irv0fTiYL_"}},{"cell_type":"code","source":["print(data_train.data[0])"],"metadata":{"id":"NZUTM3vDihak","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400109251,"user_tz":0,"elapsed":8,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"5d37b35f-b7c9-467d-f1cf-93b9b24d17cf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Well, they claim they are the only radio broadcaster with this\n","information.  But the city's cable channel (35 in CableVision areas)\n","shows this information map during travel times (6-9am and 4-7pm, I\n","believe).  Most of the major LA freeways are covered.  The\n","computer-generated map shows green, yellow, red, or flashing red\n","(respectively: <40mph, 25-40mph, >25mph, and \"incident\"--I might be off\n","a little on the speeds, since this is from memory).\n","\n","I often look at this display in the morning to see if I really want to\n","fight the traffic on the Sepulveda Pass or work from home for a little\n","while to wait for it to clear.\n","\n","Another poster explained the origin of the information: sensors (embedded\n","wire loops) in the pavement near ramps and every half mile or so.  CalTrans\n","has had a \"big board\" driven from this data in their traffic control center\n","for some time.  I don't know if they are selling the data or if anyone\n","with the equipment necessary for its transmission and display can have\n","it.\n"]}]},{"cell_type":"code","source":["print(data_train.data[199])"],"metadata":{"id":"jRX_FTvxKwSC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400109256,"user_tz":0,"elapsed":3,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"33789711-8fa3-46e3-b59f-678ad24d2a9f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Well, you really can't dig a hole with a stock Shovel; you at least need some\n","performance mods like stroking and cams.  Besides, it's REAL bad on the\n","rear tire.\n"]}]},{"cell_type":"markdown","source":["Hmm.. This seems way to neat, let's take a look what this text really looks like using the `repr` function."],"metadata":{"id":"U-NrjDXriysi"}},{"cell_type":"code","source":["print(repr(data_train.data[0]))"],"metadata":{"id":"iEfp4ayCi5Ll","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400109258,"user_tz":0,"elapsed":2,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"cfb6d5dd-c81e-4737-ae3c-12e3c647e796"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["'\\nWell, they claim they are the only radio broadcaster with this\\ninformation.  But the city\\'s cable channel (35 in CableVision areas)\\nshows this information map during travel times (6-9am and 4-7pm, I\\nbelieve).  Most of the major LA freeways are covered.  The\\ncomputer-generated map shows green, yellow, red, or flashing red\\n(respectively: <40mph, 25-40mph, >25mph, and \"incident\"--I might be off\\na little on the speeds, since this is from memory).\\n\\nI often look at this display in the morning to see if I really want to\\nfight the traffic on the Sepulveda Pass or work from home for a little\\nwhile to wait for it to clear.\\n\\nAnother poster explained the origin of the information: sensors (embedded\\nwire loops) in the pavement near ramps and every half mile or so.  CalTrans\\nhas had a \"big board\" driven from this data in their traffic control center\\nfor some time.  I don\\'t know if they are selling the data or if anyone\\nwith the equipment necessary for its transmission and display can have\\nit.'\n"]}]},{"cell_type":"markdown","source":["Ah there we are! Hopefully you can also see soe extra characters.\n","\n","All these hidden characters are going to need dealing with if we want to use this text. For those who are interested, characters such as `\\n` and `\\t` are hidden but are used to produce a formatted output. `\\n` creates a new line, `\\t` adds a tab and in this dataset `\\'` is used for apostrophes.\n","\n","Now we have taken a look at the data let's build our preprocessing pipeline!\n","\n","## Part 1: Preprocessing Pipeline\n","In order for us to process these posts we are going to need to implement the following functions:\n","+ Case Folding\n","+ Punctuation removal\n","+ Hidden character removal\n","+ Whitespace removing\n","+ Tokenisation\n","+ Stopword removal\n","+ Stemming\n","+ Lemmatisation\n","\n","We will be implementing both stemming and lemmatisation to compare later!\n","\n","> **Question:** We are implementing punctuation **and** stopword removal. Why is this okay in the given scenario?"],"metadata":{"id":"CjdWvENFi8Ay"}},{"cell_type":"markdown","source":["### Case Folding\n","We will start by lowering the case of all text in each of the texts.\n","\n","To save you time I have provided the function below along with an example of how it is used.\n"],"metadata":{"id":"6DjJ1_Dal6UI"}},{"cell_type":"code","source":["def lowercase(texts):\n","  return [t.lower() for t in texts]"],"metadata":{"id":"xH__Tb59bGWN","executionInfo":{"status":"ok","timestamp":1770400110653,"user_tz":0,"elapsed":1,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["my_texts = [\"Example PiEce of TEXT\"]\n","lowercase(my_texts)"],"metadata":{"id":"vRP-xOyYnAkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400111253,"user_tz":0,"elapsed":22,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"4d0ca533-2dd0-446e-b8e2-8028afa456e0"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['example piece of text']"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Punctuation Removal\n","Next we will start by writing a function capable of removing the punctuation from a list of strings.\n","\n","I have provided you an outline function which accepts a list of strings and returns this list following *some* processing.\n","\n","Complete the processing portion of the code between the `#### BEGIN CODE ####` and `#### END CODE ####` comments.\n","\n","To do this, you will want to make use of the `punctuation` module in the `string` library which contains all relevant items of punctuation (`from string import punctuation`).\n"],"metadata":{"id":"lfI4-vcQk2Ix"}},{"cell_type":"code","source":["from string import punctuation\n","list_punc = list(punctuation)\n","print(list_punc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otf_lHbt3pKQ","executionInfo":{"status":"ok","timestamp":1770400112475,"user_tz":0,"elapsed":12,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"07ce63d0-4e7b-46a3-c455-a40013ad67ef"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"]}]},{"cell_type":"code","source":["from string import punctuation\n","def remove_punctuation(texts):\n","\n","  list_punc = list(punctuation)\n","\n","  cleaned_texts = []\n","  #### BEGIN CODE\n","  for text in texts:\n","    for punc in list_punc:\n","      text = text.replace(punc, \"\")\n","    cleaned_texts.append(text)\n","\n","\n","  #### END CODE\n","  return cleaned_texts"],"metadata":{"id":"4Iasuct3YpGd","executionInfo":{"status":"ok","timestamp":1770400113174,"user_tz":0,"elapsed":48,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# This code can test your method\n","output = remove_punctuation([\"hello!./mynames.,<>'@notyetti\"])[0]\n","if output ==\"hellomynamesnotyetti\":\n","  print(\"Correct!\")\n","else:\n","  print(\"Failed! Double check your code\")\n","  print(\"Expected: hellomynamesnotyetti\")\n","  print(f\"Received: {output}\")"],"metadata":{"id":"D4CGE7EDqHnZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400113783,"user_tz":0,"elapsed":6,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"6cd1b6e3-f2d9-4412-c486-68a104fca945"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct!\n"]}]},{"cell_type":"markdown","source":["### Hidden Character Removal\n","Now it's time for us to remove those pesky hidden characters, a blank function has been provided for you to use. Remember we need to remove the following characters:\n","+ `\\n`\n","+ `\\t`\n","+ `\\'`\n","\n","For the purposes of this example I recommend changing all of these to an empty string.\n","\n","> **Question:** Why is it okay at this stage to replace apostrophes with an empty string?\n"],"metadata":{"id":"Yc1dLBExnSdT"}},{"cell_type":"code","source":["def remove_hidden_characters(texts):\n","  cleaned_texts = []\n","  #### BEGIN CODE\n","  for text in texts:\n","    text = text.replace(\"\\n\", \" \")\n","    text = text.replace(\"\\t\", \" \")\n","    text = text.replace(\"\\'\", \"\")\n","    cleaned_texts.append(text)\n","\n","  #### END CODE\n","  return cleaned_texts"],"metadata":{"id":"Pk9eLnMSafIe","executionInfo":{"status":"ok","timestamp":1770400115131,"user_tz":0,"elapsed":4,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# This code can test your method\n","output = remove_hidden_characters([\"hello!.\\n\\n\\t/mynames.,<>\\''@not\\\\nyetti\"])[0]\n","if output ==\"hello!.   /mynames.,<>@not\\\\nyetti\":\n","  print(\"Correct!\")\n","else:\n","  print(\"Failed! Double check your code\")\n","  print(repr(\"Expected: hello!.   /mynames.,<>@not\\nyetti\"))\n","  print(repr(f\"Received: {output}\"))"],"metadata":{"id":"szGJFiLRquTu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400115852,"user_tz":0,"elapsed":19,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"c2ee17b2-7542-4c51-b334-7a3ff76438cc"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct!\n"]}]},{"cell_type":"markdown","source":["### Whitespace Removal\n","In this example, we don't have to worry much about white space removal as the domain of each document is well defined. However, I have provided you an example function which would remove leading and trailing white space.\n"],"metadata":{"id":"wboF-RWnon7x"}},{"cell_type":"code","source":["def whitespace_removal(texts):\n","  return [t.strip() for t in texts]"],"metadata":{"id":"S4ITwBp5YpJe","executionInfo":{"status":"ok","timestamp":1770400116896,"user_tz":0,"elapsed":1,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Tokenisation\n","Now onto the fun part! It is time to break our documents down into lists of words. To do this we will make use of the `word_tokenise(...)` function from the `nltk` library (a very popular NLP library).\n","\n","Below is an example of the `word_tokenise(...)` method."],"metadata":{"id":"rwI8WeUlpBnE"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')"],"metadata":{"id":"KIilMTpuLxWo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400118378,"user_tz":0,"elapsed":445,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"8ff1eddb-c8db-4bd2-f4ae-7c277adfa99b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","word_tokenize(\"My words are being split up\")"],"metadata":{"id":"rEXVLHSdpq7O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400118417,"user_tz":0,"elapsed":33,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"db01e3bd-4102-4bbd-fb6a-0ed31ec0eea6"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['My', 'words', 'are', 'being', 'split', 'up']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["The `word_tokenize(...)` method uses two types of tokenisation, the first is a [TreeBankTokenizer](https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html#nltk.tokenize.TreebankWordTokenizer) which uses regular expressions to break text into individual units. The second is the [PunktSentenceTokenizer](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html#nltk.tokenize.PunktSentenceTokenizer) which uses word colocations to identify the beginning and end of sentences.\n","\n","Now implement the empty function below to tokenise a list of texts."],"metadata":{"id":"nID5-f1Tpy6R"}},{"cell_type":"code","source":["def tokenise(texts):\n","  tokenised_texts = []\n","  #### BEGIN CODE\n","  for text in texts:\n","    tokenised_texts.append(word_tokenize(text))\n","\n","  #### END CODE\n","  return tokenised_texts"],"metadata":{"id":"9eV_hPP6YpDx","executionInfo":{"status":"ok","timestamp":1770400119507,"user_tz":0,"elapsed":5,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# This code can test your method\n","output = tokenise([\"let me check. the method\"])[0]\n","if output ==[\"let\", \"me\", \"check\", \".\", \"the\", \"method\"]:\n","  print(\"Correct!\")\n","else:\n","  print(\"Failed! Double check your code\")\n","  print(repr(\"Expected: [let, me, check, ., the, method]\"))\n","  print(repr(f\"Received: {output}\"))"],"metadata":{"id":"S3eK6nN6rUCH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400119923,"user_tz":0,"elapsed":5,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"cdae8e8d-e9cc-4c4e-be4e-b48ee79fdec0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct!\n"]}]},{"cell_type":"markdown","source":["Whilst we are looking at tokenisers, it's worth looking at what happens if you change up the previous steps in the pipeline. For example, how would a tokeniser handle punctuation? Let's find out!"],"metadata":{"id":"gp6noDpAtw1m"}},{"cell_type":"code","source":["tokenise([\"HELP! This sentence has punctuation.\", \"How about apostrophes? She's hungry today.\"])"],"metadata":{"id":"IYz2bQiSt6YQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400120641,"user_tz":0,"elapsed":4,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"5fda5cd1-f8f1-45ac-e94f-08b575283317"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['HELP', '!', 'This', 'sentence', 'has', 'punctuation', '.'],\n"," ['How', 'about', 'apostrophes', '?', 'She', \"'s\", 'hungry', 'today', '.']]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["> **Question:** Try tokenising a few different sentences with varying degrees of punctuation. Consider whether there is a case for leaving punctuation in the sentence for tokenisation.\n","\n"],"metadata":{"id":"2yZBLwVluFJs"}},{"cell_type":"markdown","source":["### Stopword Removal\n","Before we move onto stemming and lemmatisation we will remove all the stop words from our texts.\n","\n","To do this we can again make use of a module which contains a set of stopwords in various languages in `nltk`."],"metadata":{"id":"_i_OYR_nrzal"}},{"cell_type":"code","source":["nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"id":"AYlBa_MBNlxX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400122370,"user_tz":0,"elapsed":35,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"39bb7037-394c-41ba-9376-5126e0967a48"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","print(list(stopwords.words('english')))"],"metadata":{"id":"uhW8ZCuvNUiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400123008,"user_tz":0,"elapsed":6,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"5071415d-1625-4d7f-d95b-45f6ca8c8536"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"]}]},{"cell_type":"markdown","source":["Using the empty function below remove stop words from each text."],"metadata":{"id":"rAxJt0DBNU8j"}},{"cell_type":"code","source":["\n","def remove_stopwords(texts):\n","  cleaned_texts = []\n","  stop_words = list(stopwords.words('english'))\n","  #### BEGIN CODE\n","\n","\n","  for text in texts:\n","    cleaned_sentences = []\n","    for word in text:\n","      if str(word) not in stop_words:\n","        cleaned_sentences.append(word)\n","    cleaned_texts.append(cleaned_sentences)\n","\n","  #### END CODE\n","\n","\n","\n","  return cleaned_texts"],"metadata":{"id":"ADw5O8WWYpBR","executionInfo":{"status":"ok","timestamp":1770400124047,"user_tz":0,"elapsed":3,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# This code can test your method\n","output = remove_stopwords(tokenise([\"forgetting about the rest of the universe\"]))[0]\n","if output == ['forgetting', 'rest', 'universe']:\n","  print(\"Correct!\")\n","else:\n","  print(\"Failed! Double check your code\")\n","  print(repr(\"Expected: [forgetting, rest, universe]\"))\n","  print(repr(f\"Received: {output}\"))"],"metadata":{"id":"T7aUU33ZsVWa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400124681,"user_tz":0,"elapsed":9,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"7c5801c4-e429-464e-d2b9-b69337da244b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct!\n"]}]},{"cell_type":"markdown","source":["### Stemming and Lemmatisation\n","\n","The penultimate stage of our processing pipeline! To give you a break I have implemented both a stemmer and a lemmatiser below using the nltk `PorterStemmer` and `WordNetLemmatizer`.\n","\n","There are other options available such as the `SnowballStemmer` which you should definitely try outside of the lab when you have time (you will not be assessed on the inner workings of these algorithms)."],"metadata":{"id":"L6Gk9lM0umN2"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","\n","def stem_texts(texts):\n","  stemmer = PorterStemmer()\n","  return [[stemmer.stem(word) for word in text] for text in texts]\n","\n","def lemmatise_texts(texts):\n","  lemmatiser = WordNetLemmatizer()\n","  return [[lemmatiser.lemmatize(word) for word in text] for text in texts]"],"metadata":{"id":"CRC9ksRxcTG8","executionInfo":{"status":"ok","timestamp":1770400126088,"user_tz":0,"elapsed":9,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Take a couple of minutes to try a few different words out using stemming and lemmatisation below. An example has been provided for you.\n","\n","`print(stem_texts([[<WORD 1>, <WORD 2>, <WORD 3>]]))`\n","\n","`print(lemmatise_texts([[<WORD 1>, <WORD 2>, <WORD 3>]]))`\n","\n","Here are a few words you could try:\n","+ run, running, ran\n","+ dog, puppy, kitten\n","+ Lemmatize, Stemming, Holofield"],"metadata":{"id":"k7Jxk4r3N6Fw"}},{"cell_type":"code","source":["#### BEGIN CODE\n","print(stem_texts([['run', 'running', 'ran']]))\n","print(stem_texts([['dog', 'puppy', 'kitten']]))\n","print(stem_texts([['Lemmatize', 'Stemming', 'Holofield']]))\n","\n","print(lemmatise_texts([['run', 'running', 'ran']]))\n","print(lemmatise_texts([['dog', 'puppy', 'kitten']]))\n","print(lemmatise_texts([['Lemmatize', 'Stemming', 'Holofield']]))\n","#### END CODE"],"metadata":{"id":"znuRhQSjN6eP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400131547,"user_tz":0,"elapsed":3879,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"01949759-ac97-4453-e31a-d0dfe9047143"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[['run', 'run', 'ran']]\n","[['dog', 'puppi', 'kitten']]\n","[['lemmat', 'stem', 'holofield']]\n","[['run', 'running', 'ran']]\n","[['dog', 'puppy', 'kitten']]\n","[['Lemmatize', 'Stemming', 'Holofield']]\n"]}]},{"cell_type":"markdown","source":["### Bringing it all together\n","\n","Finally we define a function to carry out all of the above activities on a list of texts."],"metadata":{"id":"M5ev4w38v1i4"}},{"cell_type":"code","source":["def preprocess(texts, reduction='s'):\n","  texts = lowercase(texts)\n","  texts = remove_punctuation(texts)\n","  texts = remove_hidden_characters(texts)\n","  texts = whitespace_removal(texts)\n","  texts = tokenise(texts)\n","  texts = remove_stopwords(texts)\n","  # Allow a choice as to whether stem or lemmtise\n","  if reduction == 's':\n","    texts = stem_texts(texts)\n","  else:\n","    texts = lemmatise_texts(texts)\n","  return texts"],"metadata":{"id":"sj0WTtJJv7tS","executionInfo":{"status":"ok","timestamp":1770400131549,"user_tz":0,"elapsed":0,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["In the space below use the function above to try out your proprocessing on a given example from the dataset!\n","\n","For example:\n","\n","`print(data_train.data[INDEX])`\n","\n","`print(preprocess([data_train.data[IDEX]]))`"],"metadata":{"id":"yENgrrylwVau"}},{"cell_type":"code","source":["#### BEGIN CODE\n","print(data_train.data[1])\n","#### END CODE"],"metadata":{"id":"GIWYZCabwU-C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400131565,"user_tz":0,"elapsed":15,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"00a47c72-3bf4-42b5-cf82-def5cf73e228"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Article 10886 of alt.radio.scanner:\n","Path: usenet.ins.cwru.edu!cleveland.Freenet.Edu!aj008\n","From: aj008@cleveland.Freenet.Edu (Aaron M. Barnes)\n","Subject: Realistic PRO-2024 for sale-was $200,sell for $150 obo\n","Date: 20 Apr 1993 16:01:28 GMT\n","Organization: Case Western Reserve University, Cleveland, Ohio (USA)\n","Lines: 26\n","Message-ID: <1r16oo$3du@usenet.INS.CWRU.Edu>\n","NNTP-Posting-Host: slc10.ins.cwru.edu\n","\n","\n","Hello.\n","\n","I have a Realistic PRO-2024 scanner for sale.Here is a small desc\n","ription:\n","\n","60 programible chanels\n","fully detailed backlighted digital display\n","headphone jack\n","antenna jack\n","removable telescoping antenna\n","auto search\n","\n","coverage:\n","30-50mHz\n","118-174mHz\n","380-512mHz\n","\n","It originally cost $200, but I will sell for $150.\n","\n","Thank You.\n","-- \n","       / /     Buchanan in `96!\n","      / /      Fear the goverment that fears your guns.\n","  \\ \\/ /       Without the 2nd amendment, we cannot guarantee ou\n","   \\/ /        r freedoms.           aj008@cleveland.freenet.edu\n"]}]},{"cell_type":"code","source":["print(preprocess([data_train.data[1]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxgBD8SNyY7x","executionInfo":{"status":"ok","timestamp":1770400132693,"user_tz":0,"elapsed":16,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"9aa6aac0-e534-4899-b819-6c5461dc52d3"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[['articl', '10886', 'altradioscann', 'path', 'usenetinscwrueduclevelandfreeneteduaj008', 'aj008clevelandfreenetedu', 'aaron', 'barn', 'subject', 'realist', 'pro2024', 'salewa', '200sell', '150', 'obo', 'date', '20', 'apr', '1993', '160128', 'gmt', 'organ', 'case', 'western', 'reserv', 'univers', 'cleveland', 'ohio', 'usa', 'line', '26', 'messageid', '1r16oo3duusenetinscwruedu', 'nntppostinghost', 'slc10inscwruedu', 'hello', 'realist', 'pro2024', 'scanner', 'saleher', 'small', 'desc', 'ription', '60', 'program', 'chanel', 'fulli', 'detail', 'backlight', 'digit', 'display', 'headphon', 'jack', 'antenna', 'jack', 'remov', 'telescop', 'antenna', 'auto', 'search', 'coverag', '3050mhz', '118174mhz', '380512mhz', 'origin', 'cost', '200', 'sell', '150', 'thank', 'buchanan', '96', 'fear', 'gover', 'fear', 'gun', 'without', '2nd', 'amend', 'guarante', 'ou', 'r', 'freedom', 'aj008clevelandfreenetedu']]\n"]}]},{"cell_type":"markdown","source":["## Part 2: Encoding\n","\n","In this part of the lab  we will encode our texts using a one-hot encoding scheme. In order to do this we will first need to build a vocabulary which we can then use to define our encoding schema."],"metadata":{"id":"CPjil-BzxB_q"}},{"cell_type":"markdown","source":["### Vocabulary\n","\n","First, let's build a vocabulary from the texts available to us. Our vocabulary should map words to a position in a list.\n","\n","For example, let's build a vocabulary for the following sentences:\n","+ \"i love dogs\" -> [i, love, dogs]\n","+ \"i like cats\" -> [i, like, cats]\n","\n","We have five distinct words so each sentence can be represented by a vector with a magnitude of 5 (e.g. [0, 0, 0, 0, 0]). But we need to define which each of these positions means, we can do this using a dictionary:\n","\n","```\n","my_vocabulary = {\n","  \"i\": 0,\n","  \"love\": 1,\n","  \"dogs\": 2,\n","  \"like\": 3,\n","  \"cats\": 4\n","}\n","```\n","I could then use this dictionary as a reference to build my feature vectors:\n","\n","```\n","feature_vector = [0] * len(my_vocabulary)\n","for word in ['i', 'love', 'dogs']:\n","  feature_vector[my_vocabulary[word]]=1\n","```\n","\n","The code above defines a list with the same size as my vocabulary, it then cycles through each word in the sentence and sets the corresponding word's value to 1 in the feature vector.\n","\n","See this code implemented below."],"metadata":{"id":"YidoohVSxSWi"}},{"cell_type":"code","source":["sentences = [['i', 'love', 'dogs'],['i', 'like', 'cats']]\n","\n","my_vocabulary = {\n","  \"i\": 0,\n","  \"love\": 1,\n","  \"dogs\": 2,\n","  \"like\": 3,\n","  \"cats\": 4\n","}\n","\n","# We are going to cycle through the sentences\n","for s in sentences:\n","  # Here's the code from above\n","  feature_vector = [0] * len(my_vocabulary)\n","  for word in s:\n","    feature_vector[my_vocabulary[word]]=1\n","  print(feature_vector)"],"metadata":{"id":"h6I8WLj-ycjh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400134947,"user_tz":0,"elapsed":6,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"c44eed82-313b-4519-9330-acfd789c52a3"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 0, 0]\n","[1, 0, 0, 1, 1]\n"]}]},{"cell_type":"markdown","source":["Assuming this worked you should see we have now converted the sentences into one-hot encoding vectors!\n","\n","Now it is your turn, using the template function below build a vocabulary we can use to encode our texts."],"metadata":{"id":"vU8zhj18zQdm"}},{"cell_type":"code","source":["def build_vocabulary(texts):\n","    vocabulary = {}\n","    c = 0\n","    #### BEGIN CODE\n","    for text in texts:\n","      for word in text:\n","        if word not in vocabulary:\n","          vocabulary[word] = c\n","          c = c + 1\n","    #### END CODE\n","    return vocabulary"],"metadata":{"id":"5_EivE7TygnO","executionInfo":{"status":"ok","timestamp":1770400136543,"user_tz":0,"elapsed":3,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# This code can test your method\n","size = len(build_vocabulary(preprocess(data_train.data[0:5])))\n","if size == 237:\n","  print(\"Correct!\")\n","else:\n","  print(\"Failed! Double check your code\")\n","  print(repr(\"Expected a vocabulary size of: 237\"))\n","  print(repr(f\"Output had a size of: {size}\"))"],"metadata":{"id":"0B8BByAEzhTT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400137807,"user_tz":0,"elapsed":19,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"7e3263ef-9c87-4fcc-f299-cfee31d71446"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct!\n"]}]},{"cell_type":"markdown","source":["> **Question:** What influence will the size of vocabulary have on the size of the feature vectors created?\n","\n","> **Question:** What would happen if a word appears in the test dataset which wasn't a part of the vocabulary?"],"metadata":{"id":"M7P6KNEs0-Vg"}},{"cell_type":"markdown","source":["### Encoding\n","\n","Great! Now here's an encoding function prewritten for you which will use your vocabulary builder from above, it will return a numpy matrix for ease-of-use, take some time to understand what it is doing."],"metadata":{"id":"OdenuBJR0BCY"}},{"cell_type":"code","source":["import numpy as np\n","def encode(texts, vocabulary=None):\n","  vocabulary = build_vocabulary(texts) if vocabulary is None else vocabulary\n","  dataset = []\n","  for t in texts:\n","    feature_vector = [0] * len(vocabulary)\n","    for w in t:\n","      if w in vocabulary:\n","        feature_vector[vocabulary[w]] = 1\n","    dataset.append(feature_vector)\n","  return np.array(dataset), vocabulary"],"metadata":{"id":"NzW_PT7xfyEw","executionInfo":{"status":"ok","timestamp":1770400422845,"user_tz":0,"elapsed":41,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["## Part 3: Building a Classifier\n","\n","It's time to try out our one-hot vectors at a classification problem!\n","\n","The code below will generate a feature set for the training data using stemming.\n","\n","(It will take more than a few seconds, if you want to reduce the size of the training set change the `data_train.data` parameter to `data_train.data[:500]` for the first 500 items.)"],"metadata":{"id":"rhlzdKFh1n-m"}},{"cell_type":"code","source":["train_features, train_vocabulary = encode(preprocess(data_train.data, reduction='s'))"],"metadata":{"id":"OcnzwGmY1nax","executionInfo":{"status":"ok","timestamp":1770400466894,"user_tz":0,"elapsed":11274,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["train_targets = data_train.target"],"metadata":{"id":"Huk4XcLk2z-7","executionInfo":{"status":"ok","timestamp":1770400466898,"user_tz":0,"elapsed":1,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["Now you have your training features and target labels train a logistic regression model below, the LogisticRegression model from sklearn has already been imported for you."],"metadata":{"id":"l917tBLz2fCT"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","#### BEGIN CODE\n","clf = LogisticRegression(random_state=0).fit(train_features, train_targets)\n","#### END CODE"],"metadata":{"id":"Kskb0Z1j1nYE","executionInfo":{"status":"ok","timestamp":1770400626924,"user_tz":0,"elapsed":13205,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["It's now time to evaluate how well your classifier has done on an unseen dataset. The `precision_score`, `recall_score` and `accuracy_score` methods have been imported and a test dataset has been loaded for you below.\n","\n","Again ifyou wish to use a smaller test set for debugging add a `[:100]` qualifier to the test_features and test_targets definitions."],"metadata":{"id":"QlTc0YL83qFW"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, accuracy_score\n","\n","# Retrieve the test data\n","data_test = fetch_20newsgroups(\n","    subset=\"test\",\n","    categories=[\n","      'rec.motorcycles',\n","      'rec.sport.hockey',\n","      'sci.electronics',\n","      'sci.space'\n","    ],\n","    shuffle=True,\n","    random_state=42,\n","    remove=(\"headers\", \"footers\", \"quotes\"),\n",")\n","\n","# Encode the test data\n","test_features, _ = encode(preprocess(data_test.data, reduction='s'), vocabulary=train_vocabulary)\n","test_targets = data_test.target"],"metadata":{"id":"EmH2PXVt3yVP","executionInfo":{"status":"ok","timestamp":1770402382129,"user_tz":0,"elapsed":16942,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["Using this test dataset and the imported methods evaluate your classifier."],"metadata":{"id":"eVLfLGwq3ua4"}},{"cell_type":"code","source":["#### BEGIN CODE\n","\n","y_pred = clf.predict(test_features)\n","print(f\"Accuracy: {accuracy_score(test_targets, y_pred)}\")\n","print(f\"Precision: {precision_score(test_targets, y_pred, average='macro')}\")\n","print(f\"Recall: {recall_score(test_targets, y_pred, average='macro')}\")\n","#### END CODE"],"metadata":{"id":"4ZRtl8Xc1nVg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770400719218,"user_tz":0,"elapsed":626,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"f43fbbbc-cdc0-4e35-96b3-a43f05a05a66"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8257575757575758\n","Precision: 0.8321984865050357\n","Recall: 0.8254251576505645\n"]}]},{"cell_type":"markdown","source":["## Part 4: Comparing Stemming and Lemmatisation\n","\n","Well done on making it this far!\n","\n","In the space below create a seperate classifier using lemmatisation and compare its results to your stemming classifier.\n","\n","REMEMBER: Your vocabularies will be different to the stemmer!\n","\n","Start by encoding training and testing features using lemmatisation."],"metadata":{"id":"LyhRkx145bpi"}},{"cell_type":"code","source":["#### BEGIN CODE\n","train_features, train_vocabulary = encode(preprocess(data_train.data, reduction='l'), vocabulary = None)\n","test_features, _ = encode(preprocess(data_test.data, reduction='l'), vocabulary=train_vocabulary)\n","#### END CODE"],"metadata":{"id":"UERotgV753UH","executionInfo":{"status":"ok","timestamp":1770403108999,"user_tz":0,"elapsed":12920,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["Now use the lemmatised features to create a new `LogisticRegression` model and evaluate it using the test features."],"metadata":{"id":"jqmd2vQeRKYz"}},{"cell_type":"code","source":["#### BEGIN CODE\n","clf = LogisticRegression(random_state=0).fit(train_features, train_targets)\n","\n","lemma_pred = clf.predict(test_features)\n","print(f\"Accuracy: {accuracy_score(test_targets, lemma_pred)}\")\n","print(f\"Precision: {precision_score(test_targets, lemma_pred, average='macro')}\")\n","print(f\"Recall: {recall_score(test_targets, lemma_pred, average='macro')}\")\n","#### END CODE"],"metadata":{"id":"3TC6ZTIs5bJB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770403170911,"user_tz":0,"elapsed":16276,"user":{"displayName":"Kamal Israni","userId":"14634172021986565173"}},"outputId":"fa3c4f14-27e4-470e-d19e-a7c053d2dee6"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8295454545454546\n","Precision: 0.8352602257734841\n","Recall: 0.8292051005770353\n"]}]},{"cell_type":"markdown","source":["## Part 5 (optional): Expand!\n","\n","If you have time and want to trial the same experiment on a larger corpus, you can remove the categories parameter in the calls to `fetch_20newsgroups` this will retrieve 20 different categories of posts for you to test the classifier on!"],"metadata":{"id":"Eu6sOtFP6T-3"}},{"cell_type":"markdown","source":["## Wrap-up\n","\n","I hope you enjoyed this lab and found the implementation elements useful for understanding what we doing to the text and why!\n","\n","As you can tell there are a lot of options here depending on the domain you are working in and it is too much to fit into one lab but I recommend foing some research into the steps involved in web preprocessing or email processing.\n","\n","That's the end of week 1, I look forward to seeing you next week!"],"metadata":{"id":"zeru8w3f6qVQ"}}]}