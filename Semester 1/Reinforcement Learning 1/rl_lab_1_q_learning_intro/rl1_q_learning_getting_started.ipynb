{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent-Environment Interaction: Getting Started\n",
    "\n",
    "This notebook presents _one possible_ approach to get you started in the agent-environment interaction exercise. You are highly encouraged to make your own decisions and deviate from this template - the more you do on your own, the more you will learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General structure\n",
    "\n",
    "We want to implement the agent-environment interface depicted in the exercise sheet. We need\n",
    "* an **agent** with a random policy,\n",
    "* an **environment**,\n",
    "* and a procedure that lets the agent interact within the environment.\n",
    "\n",
    "We will build each of these components incrementally and components as they are needed. We make the choice of defining a **class** for both the agent and the environment. \n",
    "\n",
    "For the learning procedure itself, we would eventually organise the code in a double (nested) loop; the outer one for episodes, and the inner one for steps within each episode. However, we will start looking at just **one single step**. In this step, we need to\n",
    "1. observe the current state of the environment,\n",
    "2. let the agent choose an action, and\n",
    "3. let the environment respond by changing its state and returning a reward.\n",
    "\n",
    "Written differently for one time step `t`, we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_t = environment.get_current_state()\n",
    "# action_t = agent.choose_action(state_t)\n",
    "# reward_t, state_t_plus_1 = environment.make_step(action_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus need an agent with a `choose_action(state_t)` method.\n",
    "And we need an environment with a `get_current_state()` method and a `make_step(action_t)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment: a `Gridworld` class\n",
    "\n",
    "We will distinguish between the internal state of the environment, which is hidden from the agent and the interface, and the external state, which corresponds to all the parts that are accessible to the agent. For example, we do not want the agent to know where the bomb is, but the agent has to know which actions are available.\n",
    "\n",
    "#### The internal state of the environment\n",
    "The Gridworld class has to keep track of\n",
    "* the general properties of the environment, e.g., the available actions or the number of cells\n",
    "* the positions of the agent, and both terminal states: the gold and the bomb\n",
    "* the rewards for each cell\n",
    "\n",
    "In order to keep things simple, we will implement a very unflexible version of the environment. For example, we will keep the dimensions of the grid, the number of bombs, and the position of the gold fixed. You can (read: should) introduce variables for all these hard-coded properties once your algorithm is working.\n",
    "\n",
    "Let us first define a class that specifies all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_cells = self.num_cols * self.num_rows\n",
    "        self.random_move_probability = 0.2\n",
    "        \n",
    "        # Choose starting position of the agent randomly among the first 5 cells\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "        \n",
    "        # Choose position of the gold and bomb\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.terminal_states = np.array([self.bomb_positions, self.gold_positions])\n",
    "       \n",
    "        # Specify rewards\n",
    "        self.rewards = np.zeros(self.num_cells)\n",
    "        self.rewards[self.bomb_positions] = -10\n",
    "        self.rewards[self.gold_positions] = 10\n",
    "        \n",
    "        # Specify available actions\n",
    "        self.actions = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "    \n",
    "    ## Put other methods here:\n",
    "    # def reset():\n",
    "    #     ...\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we chose to specify the cells in a one-dimensional array instead of a two-dimensional array (e.g., the bomb is at position `18` and not at position `[3,3]` in a two-dimensional array). This is a completely personal choice and you could choose either option. \n",
    "\n",
    "#### The interface of the environment\n",
    "We now want to define a method that provides any agent with the needed (and allowed!) information about the current state of the environment.\n",
    "\n",
    "Take a minute and think about which fields among those we defined above are needed both for the RandomAgent and, later, the Q-learning agent. \n",
    "\n",
    "...? You found an answer? Go on.\n",
    "\n",
    "The RandomAgent only needs the available actions (technically, only the number of available actions) to randomly choose from. The Q-learning agent will need the available actions _and_ its current position.\n",
    "\n",
    "Let us for now focus on the random agent and define a method for the Gridworld class that returns the available actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the following method to the Gridworld class definition above and re-execute \n",
    "## that cell to redefine the Gridworld class. Be careful to include the right amount\n",
    "## of indentation.\n",
    "\n",
    "    # ...\n",
    "    def get_available_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "## You can delete this cell after having updated the Gridworld class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent: a `RandomAgent` class\n",
    "\n",
    "We now define a RandomAgent class with a `choose_action` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def choose_action(self, available_actions):\n",
    "        number_of_actions = len(available_actions)\n",
    "        random_action_index = np.random.randint(0, number_of_actions)\n",
    "        return random_action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code that we've written so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "agent = RandomAgent()\n",
    "\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the agent has chosen a random action, we need the environment to respond to that action. Let us define another method for the `Gridworld` class that takes the chosen action as input, updates the internal state of the environment, and returns a reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Add to Gridworld class definition\n",
    "    \n",
    "    #....\n",
    "    def make_step(self, action_index): \n",
    "        # Randomly sample action_index if world is stochastic\n",
    "        if np.random.uniform(0, 1) < self.random_move_probability:\n",
    "            action_indices = np.arange(self.num_actions, dtype=int)\n",
    "            action_indices = np.delete(action_indices, action_index)\n",
    "            action_index = np.random.choice(action_indices, 1)[0]\n",
    "\n",
    "        action = self.actions[action_index]\n",
    "\n",
    "        # Determine new position and check whether the agent hits a wall.\n",
    "        old_position = self.agent_position\n",
    "        new_position = self.agent_position\n",
    "        if action == \"UP\":\n",
    "            candidate_position = old_position + self.num_cols\n",
    "            if candidate_position < self.num_cells:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"RIGHT\":\n",
    "            candidate_position = old_position + 1\n",
    "            if candidate_position % self.num_cols > 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                new_position = candidate_position\n",
    "        elif action == \"DOWN\":\n",
    "            candidate_position = old_position - self.num_cols\n",
    "            if candidate_position >= 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"LEFT\":  # \"LEFT\"\n",
    "            candidate_position = old_position - 1\n",
    "            if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "                new_position = candidate_position\n",
    "        else:\n",
    "            raise ValueError('Action was mis-specified!')\n",
    "\n",
    "        # Update the environment state\n",
    "        self.agent_position = new_position\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self.rewards[self.agent_position]\n",
    "        reward -= 1\n",
    "        return reward, new_position\n",
    "\n",
    "    \n",
    "## You can delete this cell after having updated the Gridworld class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having updated the Gridworld class with the `make_step` method, create a new Gridworld..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = Gridworld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and execute the following cell multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current position of the agent =\", env.agent_position)\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)\n",
    "reward = env.make_step(chosen_action)\n",
    "print(\"Reward obtained =\", reward)\n",
    "print(\"Current position of the agent =\", env.agent_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now manually walk the RandomAgent through our gridworld!\n",
    "\n",
    "Next steps include:\n",
    "* Build a loop so that you do not have to manually move the agent.\n",
    "* Define a new class that implements a Q-learning Agent\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
