{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent-Environment Interaction\n",
    "\n",
    "In this exercise, you will implement the interaction of a reinforecment learning agent with its environment. We will use the gridworld environment from the second lecture. You will find a description of the environment below, along with two pieces of relevant material from the lectures: the agent-environment interface and the Q-learning algorithm.\n",
    "\n",
    "1. Create an agent that chooses actions randomly with this environment. \n",
    "\n",
    "2. Create an agent that uses Q-learning. You can use initial Q values of 0, a stochasticity parameter for the $\\epsilon$-greedy policy function $\\epsilon=0.05$, and a learning rate $\\alpha = 0.1$. But feel free to experiment with other settings of these three parameters.\n",
    "\n",
    "3. Plot the mean total reward (i.e. the undiscounted return) obtained by the two agents for each episode. This kind of graph is called a **learning curve**, and it gives us an idea of how our agent's performance changes during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## The agent-environment interface\n",
    "\n",
    "<img src=\"img/agent-environment.png\" style=\"width: 500px;\" align=\"left\"/> \n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment: Navigation in a gridworld\n",
    "\n",
    "<img src=\"img/gold.png\" style=\"width: 250px;\" align=\"left\"/>\n",
    "\n",
    "The agent has four possible actions in each state (grid square): west, north, south, and east. The actions are unreliable. They move the agent in the intended direction with probability 0.8, and with probability 0.2, they move the agent in a random other direction. If the direction of movement is blocked, the agent remains in the same grid square. The initial state of the agent is one of the five grid squares at the bottom, selected randomly. The grid squares with the gold and the bomb are **terminal states**. If the agent finds itself in one of these squares, the episode ends. Then a new episode begins with the agent at a randomly selected initial state.\n",
    "\n",
    "You will use a reinforcement learning algorithm to compute the best policy for finding the gold with as few steps as possible while avoiding the bomb. For this, we will use the following reward function: $-1$ for each navigation action, an additional $+10$ for finding the gold, and an additional $-10$ for hitting the bomb. For example, the immediate reward for transitioning into the square with the gold is $-1 + 10 = +9$. Do not use discounting (that is, set $\\gamma=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Q-Learning\n",
    "For your reference, the pseudocode for the Q-Learning algorithm is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.5 p.131).\n",
    "<img src=\"img/q.png\" style=\"width: 720px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Example of a learning curve\n",
    "\n",
    "<img src=\"img/lc_example.png\" style=\"width: 550px;\" align=\"left\"/>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "This is a sample learning curve and shows the reward obtained by a Q-learning agent across 500 episodes. Do not try to replicate this exact curve! It was computed using a different environment than the one described here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epsilon = 0.05\n",
    "q = 0\n",
    "\n",
    "bomb_position = 18\n",
    "gold_position = 23\n",
    "\n",
    "actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "\n",
    "probability_intended = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 5\n",
    "num_cells = num_rows*num_cols\n",
    "random_move_probability = 0.2\n",
    "\n",
    "agent_position = np.random.randint(0, 5)\n",
    "bomb_positions = np.array([18])\n",
    "gold_positions = np.array([23])\n",
    "\n",
    "terminal_states = np.array([bomb_positions, gold_positions])\n",
    "\n",
    "rewards = np.zeros(num_cells)\n",
    "rewards[bomb_positions] = -10\n",
    "rewards[gold_positions] = 10\n",
    "\n",
    "actions = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "num_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18]\n",
      "[23]\n",
      "[[18]\n",
      " [23]]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. -10.   0.   0.   0.   0.  10.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(bomb_positions)\n",
    "print(gold_positions)\n",
    "print(terminal_states)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_position = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_position = agent_position\n",
    "new_position = agent_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"UP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == \"UP\":\n",
    "    candidate_position = old_position + num_cols\n",
    "    if candidate_position < num_cells:\n",
    "        new_position = candidate_position\n",
    "elif action == \"RIGHT\":\n",
    "    candidate_position = old_position + 1\n",
    "    if candidate_position % num_cols > 0:\n",
    "        new_position = candidate_position\n",
    "elif action == \"DOWN\":\n",
    "    candidate_position = old_position - num_cols\n",
    "    if candidate_position >= 0:\n",
    "        new_position = candidate_position\n",
    "elif action == \"LEFT\":\n",
    "    candidate_position = old_position - 1\n",
    "    if candidate_position % num_cols < num_cols - 1:\n",
    "        new_position = candidate_position\n",
    "    \n",
    "        \n",
    "agent_position = new_position  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_1_n = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q_1_n = (1-learning_rate)*q_1_n + learning_rate*(reward + max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(-1,2,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
