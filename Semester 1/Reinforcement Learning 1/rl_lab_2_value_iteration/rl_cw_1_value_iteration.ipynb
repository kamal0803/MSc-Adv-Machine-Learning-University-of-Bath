{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "371f2cbb0b4a0b2e3cbc0b8203c39d28",
     "grade": false,
     "grade_id": "cell-49e38b6d0da7d1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "## Formative Assessment: Value Iteration\n",
    "In this formative exercise, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but closely related) Markov Decision Processes. For your reference, the pseudo-code for the Value Iteration algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 2018, pp. 83).\n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The symbol $r$ represents the immediate reward on transition from state $s$ to the next state $s'$ via action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 320px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The two problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this kind of environment from the content you have seen so far in this unit. The grid squares in the figure are numbered as shown. In all exercises, the following are true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are *west*, *north*, *south*, and *east*. If the direction of movement is blocked by a wall (for example, if the agent executes action south in grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold (from a neighbouring grid square), the agent collects the gold. Note that, in order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square.\n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains a bomb (from a neighbouring grid square), the agent activates the bomb. \n",
    "\n",
    "**Terminal states:** The game terminates when **all** gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to $1 \\times 10 ^{-10}$. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold (from a neighbouring grid square) is $-1 + 10 = +9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a42c1c34d1fb04bef22da8276ff9780c",
     "grade": false,
     "grade_id": "cell-bb45c706447879a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Deterministic Environment (0 Marks)\n",
    "\n",
    "In this exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your need to produce two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index $i$ representing grid cell $i$ (see figure above). Both arrays should be accessible in the \"solution cell\" below!\n",
    "\n",
    "The array `policy` should be a numpy array of strings that specifies an optimal action at each grid location. Please use the abbreviations `\"n\"`, `\"e\"`, `\"s\"`, and `\"w\"` for the four actions. As an example, the value of `policy` at index `0` needs to give `\"n\"`, if _north_ is an optimal action in cell 0. The policy for a terminal state can be any action. If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value under the optimal policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c19706e09ac17b0711f7d7b68252dfed",
     "grade": false,
     "grade_id": "cw1_value_iteration_deterministic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bomb_positions = np.array([18])\n",
    "gold_positions = np.array([23])\n",
    "rewards = np.zeros(25)\n",
    "v = np.zeros(25)\n",
    "policy = np.empty(25, dtype='<U1')\n",
    "\n",
    "theta = 1e-10\n",
    "gamma = 1\n",
    "\n",
    "rewards[18] = -10\n",
    "rewards[23] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step(agent_position, action):\n",
    "\n",
    "    old_position = agent_position\n",
    "    new_position = agent_position\n",
    "    num_cols = 5\n",
    "    num_cells = 25\n",
    "    \n",
    "    \n",
    "    if action == \"n\":\n",
    "        candidate_position = old_position + num_cols\n",
    "        if candidate_position < num_cells:\n",
    "            new_position = candidate_position\n",
    "    elif action == \"e\":\n",
    "        candidate_position = old_position + 1\n",
    "        if candidate_position % num_cols > 0:  # The %-op\n",
    "            new_position = candidate_position\n",
    "    elif action == \"s\":\n",
    "        candidate_position = old_position - num_cols\n",
    "        if candidate_position >= 0:\n",
    "            new_position = candidate_position\n",
    "    elif action == \"w\":  # \"LEFT\"\n",
    "        candidate_position = old_position - 1\n",
    "        if candidate_position % num_cols < num_cols - 1:\n",
    "            new_position = candidate_position\n",
    "        \n",
    "    agent_position = new_position\n",
    "        \n",
    "        # Calculate reward\n",
    "    reward = rewards[agent_position]\n",
    "    reward = -1 + rewards[new_position]\n",
    "    return reward, new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    delta = 0\n",
    "    v_new = np.copy(v)\n",
    "    \n",
    "    for i in range(0, 25):\n",
    "\n",
    "        if i in [18, 23]:\n",
    "            continue\n",
    "        \n",
    "        this_v = v[i]\n",
    "        q_values = []\n",
    "\n",
    "        for action in ['n', 'e', 's', 'w']:\n",
    "            reward, new_pos = make_step(i, action)\n",
    "            q_values.append(reward + gamma*v[new_pos])\n",
    "            \n",
    "        v_new[i] = max(q_values)\n",
    "        delta = max(delta, abs(v_new[i] - this_v))\n",
    "\n",
    "    v = v_new\n",
    "        \n",
    "    if delta < theta:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    if i in [18, 23]:\n",
    "        policy[i] = 'n'\n",
    "        continue\n",
    "\n",
    "    q_values = {}\n",
    "    for action in ['n', 'e', 's', 'w']:\n",
    "        r, new_pos = make_step(i, action)\n",
    "        q_values[action] = r + gamma * v[new_pos]\n",
    "\n",
    "    best_action = max(q_values, key=q_values.get)\n",
    "    policy[i] = best_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0caea5678dc94a857977196af8abb31",
     "grade": false,
     "grade_id": "cell-02a5c34a5b828d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Example Test Cell**\n",
    "\n",
    "In the code cell below, we have provided an example of the type of test code that we will use to mark your work. In these tests, we first check that your `policy` and `v` variables are of the correct type, and then check that their values match the solution. For the remaining exercises, the answers will be hidden from you until the model answers are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0721555e5b5cf97791052eab7786e27c",
     "grade": true,
     "grade_id": "cw1_value_iteration_deterministic_tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Solution policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "\n",
      "\n",
      "Student's v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n",
      "Solution v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 1 is tested here.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We're giving you the solution values for Exercise 1, but not telling you how to compute them!\n",
    "solution_values = [3.0, 4.0, 5.0, 4.0, 5.0,\n",
    "                   4.0, 5.0, 6.0, 5.0, 6.0,\n",
    "                   5.0, 6.0, 7.0, 6.0, 7.0,\n",
    "                   6.0, 7.0, 8.0, 0.0, 8.0,\n",
    "                   7.0, 8.0, 9.0, 0.0, 9.0]\n",
    "solution_values = np.array(solution_values)\n",
    "\n",
    "# We're giving you the solution policy for Exercise 1, but not telling you how to compute it!\n",
    "solution_policy = [\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'e', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'e', 'e', 'e', 'n', 'w',]\n",
    "solution_policy = np.array(solution_policy)\n",
    "\n",
    "# Check that policy and v are numpy arrays.\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.str_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether policy contains only \"n\", \"w\", \"s\", or \"e\" values.\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"]))))\n",
    "\n",
    "# Print student's solution and true solution for easier comparison / spotting of errors.\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))\n",
    "print(\"Solution policy:\")\n",
    "print(np.flip(solution_policy.reshape((5, 5)), 0))\n",
    "print(\"\\n\")\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"Solution v:\")\n",
    "print(np.flip(solution_values.reshape((5, 5)), 0))\n",
    "\n",
    "# Compare state values for non-terminal states.\n",
    "states_to_check = np.delete(np.arange(25), np.array([18, 23]))\n",
    "incorrect_error_message = \"[Marking Advice] State values incorrect.\"\n",
    "np.testing.assert_array_almost_equal(v[states_to_check], solution_values[states_to_check], decimal=3, err_msg=incorrect_error_message)\n",
    "\n",
    "# Compare policy (only on states that have a single optimal direction).\n",
    "states_to_check =  np.array([4, 9, 14, 17, 19, 20, 22, 24])\n",
    "incorrect_error_message = \"[Marking Advice] Policy incorrect.\"\n",
    "np.testing.assert_array_equal(policy[states_to_check], solution_policy[states_to_check], err_msg=incorrect_error_message)\n",
    "\n",
    "# Compare state values for terminal states.\n",
    "states_to_check =  np.array([18, 23])\n",
    "incorrect_error_message = \"[Marking Advice] Value of terminal state is not zero.\"\n",
    "np.testing.assert_array_equal(v[states_to_check], solution_values[states_to_check], err_msg=incorrect_error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ed35f6133bda044807637edec8d9fd6",
     "grade": false,
     "grade_id": "cell-24b8f994cc50fe2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Stochastic Environment\n",
    "\n",
    "In this exercise, we introduce stochasticity into the environment. Now, the agent is not always able to execute its actions as intended.\n",
    "\n",
    "With probability 0.8, the agent moves as intended. However, with probability 0.2, it moves in a random direction.\n",
    "\n",
    "For example, from grid square 0, if the agent tries to move north, with probability 0.8 the action will work as intended. But with probability 0.2, the agent's motor control system will move it in a random direction (including north). So, it will randomly try to move west, east, north or south with probability 0.05 each. Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e39b2d999356a5a459fe81749e96e67",
     "grade": false,
     "grade_id": "cw1_value_iteration_stochastic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "bomb_positions = np.array([18])\n",
    "gold_positions = np.array([23])\n",
    "rewards = np.zeros(25)\n",
    "v = np.zeros(25)\n",
    "policy = np.empty(25, dtype='<U1')\n",
    "\n",
    "theta = 1e-10\n",
    "gamma = 1\n",
    "\n",
    "rewards[18] = -10\n",
    "rewards[23] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step(agent_position, action):\n",
    "\n",
    "    old_position = agent_position\n",
    "    new_position = agent_position\n",
    "    num_cols = 5\n",
    "    num_cells = 25\n",
    "    \n",
    "    \n",
    "    if action == \"n\":\n",
    "        candidate_position = old_position + num_cols\n",
    "        if candidate_position < num_cells:\n",
    "            new_position = candidate_position\n",
    "    elif action == \"e\":\n",
    "        candidate_position = old_position + 1\n",
    "        if candidate_position % num_cols > 0:  # The %-op\n",
    "            new_position = candidate_position\n",
    "    elif action == \"s\":\n",
    "        candidate_position = old_position - num_cols\n",
    "        if candidate_position >= 0:\n",
    "            new_position = candidate_position\n",
    "    elif action == \"w\":  # \"LEFT\"\n",
    "        candidate_position = old_position - 1\n",
    "        if candidate_position % num_cols < num_cols - 1:\n",
    "            new_position = candidate_position\n",
    "        \n",
    "    agent_position = new_position\n",
    "        \n",
    "        # Calculate reward\n",
    "    reward = -1 + rewards[new_position]\n",
    "    return reward, new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    delta = 0\n",
    "    v_new = np.copy(v)\n",
    "    \n",
    "    for i in range(0, 25):\n",
    "\n",
    "        if i in [18, 23]:\n",
    "            continue\n",
    "        \n",
    "        this_v = v[i]\n",
    "        q_values = []\n",
    "\n",
    "        for action in ['n', 'e', 's', 'w']:\n",
    "            exp_value = 0\n",
    "            for actual_action in ['n', 'e', 's', 'w']:\n",
    "                if action == actual_action:\n",
    "                    prob = 0.85\n",
    "                else:\n",
    "                    prob = 0.05\n",
    "                reward, new_pos = make_step(i, actual_action)\n",
    "                exp_value = exp_value + prob*(reward + gamma*v[new_pos])\n",
    "            q_values.append(exp_value)\n",
    "            \n",
    "        v_new[i] = max(q_values)\n",
    "        delta = max(delta, abs(v_new[i] - this_v))\n",
    "\n",
    "    v = v_new\n",
    "        \n",
    "    if delta < theta:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    if i in [18, 23]:\n",
    "        policy[i] = 'n'\n",
    "        continue\n",
    "\n",
    "    q_values = {}\n",
    "    for action in ['n', 'e', 's', 'w']:\n",
    "        exp_value = 0\n",
    "        for actual_action in ['n', 'e', 's', 'w']:\n",
    "            if action == actual_action:\n",
    "                prob = 0.85\n",
    "            else:\n",
    "                prob = 0.05\n",
    "            reward, new_pos = make_step(i, actual_action)\n",
    "            exp_value = exp_value + prob*(reward + gamma*v[new_pos])\n",
    "        q_values[action] = exp_value\n",
    "\n",
    "    best_action = max(q_values, key=q_values.get)\n",
    "    policy[i] = best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f74c2390fd78c29fbaa9041639aab861",
     "grade": true,
     "grade_id": "cw1_value_iteration_stochastic_tests_1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's v:\n",
      "[[6.04169329 7.28756636 8.61359951 0.         8.69262311]\n",
      " [4.86185111 5.99087587 6.37082431 0.         6.46721593]\n",
      " [3.67550938 4.69621388 4.99441863 3.2189158  5.10250988]\n",
      " [2.48699534 3.40945989 3.66922967 2.64122933 3.78610115]\n",
      " [1.35979208 2.19733672 2.42878751 1.57272161 2.55202451]]\n",
      "\n",
      "\n",
      "Student's policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"\\n\")\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45507ff59702c530cadf05e4a75a0d91",
     "grade": false,
     "grade_id": "cell-e0de56802818cf9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Stochastic Environment with Two Pieces of Gold (6 Marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this exercise, we have modified the environment presented in exercise 2. A second piece of gold has been placed on grid square 12, and a terminal state is reached only when **both** pieces of gold have been collected, or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy for this altered environment using Value Iteration.\n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in the previous exercises. Each value in these arrays should specify the expected return and optimal policy at the corresponding grid sqaure **before any pieces of gold are collected or a bomb is activated.** \n",
    "\n",
    "Hint: You will need to change your state representation in order to account for the additional piece of gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8ba98477adae2eaf9d8c007241dce2a",
     "grade": false,
     "grade_id": "cw1_value_iteration_two_gold",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "bomb_positions = np.array([18])\n",
    "gold_positions = np.array([12, 23])\n",
    "rewards = np.zeros(25)\n",
    "v = np.zeros(25)\n",
    "policy = np.empty(25, dtype='<U1')\n",
    "\n",
    "theta = 1e-10\n",
    "gamma = 1\n",
    "\n",
    "rewards[12] = 10\n",
    "rewards[18] = -10\n",
    "rewards[23] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step(agent_position, action):\n",
    "\n",
    "    old_position = agent_position\n",
    "    new_position = agent_position\n",
    "    num_cols = 5\n",
    "    num_cells = 25\n",
    "    \n",
    "    \n",
    "    if action == \"n\":\n",
    "        candidate_position = old_position + num_cols\n",
    "        if candidate_position < num_cells:\n",
    "            new_position = candidate_position\n",
    "    elif action == \"e\":\n",
    "        candidate_position = old_position + 1\n",
    "        if candidate_position % num_cols > 0:  # The %-op\n",
    "            new_position = candidate_position\n",
    "    elif action == \"s\":\n",
    "        candidate_position = old_position - num_cols\n",
    "        if candidate_position >= 0:\n",
    "            new_position = candidate_position\n",
    "    elif action == \"w\":  # \"LEFT\"\n",
    "        candidate_position = old_position - 1\n",
    "        if candidate_position % num_cols < num_cols - 1:\n",
    "            new_position = candidate_position\n",
    "        \n",
    "    agent_position = new_position\n",
    "        \n",
    "        # Calculate reward\n",
    "    reward = -1 + rewards[new_position]\n",
    "    return reward, new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    delta = 0\n",
    "    v_new = np.copy(v)\n",
    "    \n",
    "    for i in range(0, 25):\n",
    "\n",
    "        if i in [12, 18, 23]:\n",
    "            continue\n",
    "        \n",
    "        this_v = v[i]\n",
    "        q_values = []\n",
    "\n",
    "        for action in ['n', 'e', 's', 'w']:\n",
    "            exp_value = 0\n",
    "            for actual_action in ['n', 'e', 's', 'w']:\n",
    "                if action == actual_action:\n",
    "                    prob = 0.85\n",
    "                else:\n",
    "                    prob = 0.05\n",
    "                reward, new_pos = make_step(i, actual_action)\n",
    "                exp_value = exp_value + prob*(reward + gamma*v[new_pos])\n",
    "            q_values.append(exp_value)\n",
    "            \n",
    "        v_new[i] = max(q_values)\n",
    "        delta = max(delta, abs(v_new[i] - this_v))\n",
    "\n",
    "    v = v_new\n",
    "        \n",
    "    if delta < theta:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    if i in [12, 18, 23]:\n",
    "        policy[i] = 'n'\n",
    "        continue\n",
    "\n",
    "    q_values = {}\n",
    "    for action in ['n', 'e', 's', 'w']:\n",
    "        exp_value = 0\n",
    "        for actual_action in ['n', 'e', 's', 'w']:\n",
    "            if action == actual_action:\n",
    "                prob = 0.85\n",
    "            else:\n",
    "                prob = 0.05\n",
    "            reward, new_pos = make_step(i, actual_action)\n",
    "            exp_value = exp_value + prob*(reward + gamma*v[new_pos])\n",
    "        q_values[action] = exp_value\n",
    "\n",
    "    best_action = max(q_values, key=q_values.get)\n",
    "    policy[i] = best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43e1fe235001ee4eccea7ab6e93905df",
     "grade": true,
     "grade_id": "cw1_value_iteration_two_gold_tests_1",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's v:\n",
      "[[6.27185527 7.4483022  8.69750488 0.         8.69690012]\n",
      " [6.27225747 7.38830369 7.80429043 0.         6.5442022 ]\n",
      " [7.29987383 8.60242493 0.         7.68979128 6.49253977]\n",
      " [6.18412144 7.36032106 8.59757988 7.30328591 6.08760162]\n",
      " [5.07297558 6.18349601 7.2879906  6.12946745 5.01603046]]\n",
      "\n",
      "\n",
      "Student's policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['e' 's' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['e' 'n' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'w']]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"\\n\")\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
