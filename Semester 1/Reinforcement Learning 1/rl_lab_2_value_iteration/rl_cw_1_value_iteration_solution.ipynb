{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "371f2cbb0b4a0b2e3cbc0b8203c39d28",
     "grade": false,
     "grade_id": "cell-49e38b6d0da7d1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "## Formative Assessment: Value Iteration\n",
    "In this formative exercise, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but closely related) Markov Decision Processes. For your reference, the pseudo-code for the Value Iteration algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 2018, pp. 83).\n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The symbol $r$ represents the immediate reward on transition from state $s$ to the next state $s'$ via action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 320px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The two problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this kind of environment from the content you have seen so far in this unit. The grid squares in the figure are numbered as shown. In all exercises, the following are true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are *west*, *north*, *south*, and *east*. If the direction of movement is blocked by a wall (for example, if the agent executes action south in grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold (from a neighbouring grid square), the agent collects the gold. Note that, in order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square.\n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains a bomb (from a neighbouring grid square), the agent activates the bomb. \n",
    "\n",
    "**Terminal states:** The game terminates when **all** gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to $1 \\times 10 ^{-10}$. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold (from a neighbouring grid square) is $-1 + 10 = +9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a42c1c34d1fb04bef22da8276ff9780c",
     "grade": false,
     "grade_id": "cell-bb45c706447879a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Deterministic Environment (0 Marks)\n",
    "\n",
    "In this exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your need to produce two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index $i$ representing grid cell $i$ (see figure above). Both arrays should be accessible in the \"solution cell\" below!\n",
    "\n",
    "The array `policy` should be a numpy array of strings that specifies an optimal action at each grid location. Please use the abbreviations `\"n\"`, `\"e\"`, `\"s\"`, and `\"w\"` for the four actions. As an example, the value of `policy` at index `0` needs to give `\"n\"`, if _north_ is an optimal action in cell 0. The policy for a terminal state can be any action. If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value under the optimal policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "action_names = [\n",
    "    'North',\n",
    "    'East',\n",
    "    'South',\n",
    "    'West',\n",
    "]\n",
    "\n",
    "actions = [\n",
    "    np.array([1, 0]), # North\n",
    "    np.array([0, 1]), # East\n",
    "    np.array([-1, 0]),# South\n",
    "    np.array([0, -1]),# West\n",
    "]\n",
    "\n",
    "action_abrev = [\n",
    "    'n',\n",
    "    'e',\n",
    "    's',\n",
    "    'w',\n",
    "]\n",
    "\n",
    "\n",
    "def print_state_dynamics(transition_dynamics, state):\n",
    "    dynamics = transition_dynamics[state]\n",
    "    \n",
    "    print('State {}'.format(state))\n",
    "    \n",
    "    for action, action_dynamics in dynamics.items():\n",
    "        print('\\t{}'.format(action_names[action]))\n",
    "        for actual_action, action_probs in action_dynamics.items():\n",
    "            print('\\t\\t{} - {}'.format(action_names[actual_action], action_probs))\n",
    "\n",
    "def print_transition_dynamics(transition_dynamics):\n",
    "    \"\"\"\n",
    "    Use this function if you would like to visualise the transition dynamics.\n",
    "    \n",
    "    Call this function with the transition dynamics as a parameter.\n",
    "    \"\"\"\n",
    "    for state, dynamics in transition_dynamics.items():\n",
    "        print_state_dynamics(transition_dynamics, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_dynamics(num_states, gold_states, terminal_states, bomb_states, stochasticity=1):\n",
    "    \"\"\"\n",
    "    Function to create the transition dynamics for each of the different gridworld problems.\n",
    "    Returns a dictionary of the following format.\n",
    "    'state': [\n",
    "        'north_action': [\n",
    "            'north_action': (probability, new_state, reward)\n",
    "            'east_action':  (probability, new_state, reward)\n",
    "            'south_action': (probability, new_state, reward)\n",
    "            'west_action':  (probability, new_state, reward)\n",
    "        ],\n",
    "        'east_action': [\n",
    "            'north_action': (probability, new_state, reward)\n",
    "            'east_action':  (probability, new_state, reward)\n",
    "            'south_action': (probability, new_state, reward)\n",
    "            'west_action':  (probability, new_state, reward)\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    The dictionary has one entry for every state. Each state has the four actions an agent could choose, \n",
    "    and each action has each of the 4 actions that could actually be taken if the environment is stocastic.\n",
    "    This means the dicationary can be used to lookup the transition dynamics for a given state, action pair. \n",
    "    Essentially, this function implements the p(s',r|s,a).\n",
    "    \n",
    "    We can lookup the values like so:\n",
    "    \n",
    "        transition_dynamics[state][action]\n",
    "        \n",
    "    to get the list of possible transitions this can result in and their associated probabilities:\n",
    "        \n",
    "        'north_action': (probability, new_state, reward)\n",
    "        'east_action':  (probability, new_state, reward)\n",
    "        'south_action': (probability, new_state, reward)\n",
    "        'west_action':  (probability, new_state, reward)\n",
    "    \n",
    "    This allows use to iterate over each of the potential outcomes of taking an action in a state and weight\n",
    "    them by their probability of occuring.\n",
    "    \"\"\"\n",
    "    action_reward = -1\n",
    "    gold_reward = 10\n",
    "    bomb_reward = -10\n",
    "\n",
    "    transition_dynamics = {}\n",
    "    random_action_probability = (1-stochasticity)/len(actions)\n",
    "\n",
    "    for s in range(num_states):\n",
    "        # The state multiplier is used to keep the equations consistent in the case where we\n",
    "        # have more than one gold. We use the state_multiplier to move the boards found out\n",
    "        # states 25:50 and 50:75 back to 0:25 for ease/consistency of calculation.\n",
    "        state_multiplier = s//25\n",
    "        state_offset = state_multiplier * 25\n",
    "        offset_s = s - state_offset\n",
    "        x = int(offset_s/5)\n",
    "        y = int(offset_s - (x * 5))\n",
    "        pos = np.array([x, y])\n",
    "\n",
    "        action_transitions = {}\n",
    "        \n",
    "        # For each action in a state, calculate all of its possible outcomes\n",
    "        for action in range(len(actions)):\n",
    "            action_transition_dynamics = {}\n",
    "            \n",
    "            # When taking an action iterate over each of the outcomes that could actually happen.\n",
    "            # Use this for stocastic transitions.\n",
    "            for final_direction in range(len(actions)):\n",
    "                \n",
    "                # Calculate the new state the action results in and clip the outcome to between 0 and 4.\n",
    "                # The clipping is to stop the transitions traversing of the edge of the board.\n",
    "                direction = actions[final_direction]\n",
    "                new_position = pos + direction\n",
    "                new_position = np.clip(new_position, 0, 4)\n",
    "\n",
    "                action_probability = random_action_probability\n",
    "                if action == final_direction:\n",
    "                    action_probability += stochasticity\n",
    "                \n",
    "                # Convert x,y positions back to a single state id.\n",
    "                new_state = (new_position[0] * 5) + new_position[1]\n",
    "                \n",
    "                # Caculate the reward for the transition that the state, action and new_state give.\n",
    "                # We need to translate the states back by their offset we calculated at the beginning.\n",
    "                # This allows us to still use bomb at state 67 instead of keeping track of which board\n",
    "                # configuration we are in for the case where we have 2 golds.\n",
    "                reward = action_reward\n",
    "                if new_state + state_offset in bomb_states:\n",
    "                    reward += bomb_reward\n",
    "                elif new_state + state_offset in gold_states and s != new_state:\n",
    "                    reward += gold_reward\n",
    "                \n",
    "                # If we are dealing with the two gold scenario check if we are in a gold state on the board\n",
    "                # with 2 gold pieces and adject their future states to be on the new boards.\n",
    "                # See the first few slides of the lecture for a visual explanation of this.\n",
    "                if num_states > 25:\n",
    "                    if new_state + state_offset == 12:\n",
    "                        new_state = 12 + 25\n",
    "\n",
    "                    if new_state + state_offset == 23 and s != 23:\n",
    "                        new_state = 23 + 50\n",
    "\n",
    "                new_state += state_offset\n",
    "                \n",
    "                # Create and store the transition\n",
    "                transition = (round(action_probability, 2), new_state, reward)\n",
    "\n",
    "                action_transition_dynamics[final_direction] = transition\n",
    "            action_transitions[action] = action_transition_dynamics\n",
    "        transition_dynamics[s] = action_transitions\n",
    "    return transition_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c19706e09ac17b0711f7d7b68252dfed",
     "grade": false,
     "grade_id": "cw1_value_iteration_deterministic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "\n",
    "num_states = 25\n",
    "\n",
    "gold_states = [23]\n",
    "terminal_states = [18, 23]\n",
    "bomb_states = [18]\n",
    "\n",
    "stochasticity = 1\n",
    "\n",
    "theta = 1e-10\n",
    "gamma = 1 # Discount Factor\n",
    "\n",
    "# Get the transition dynamics for the current problem.\n",
    "transition_dynamics = create_transition_dynamics(num_states, gold_states, terminal_states, bomb_states, stochasticity)\n",
    "\n",
    "\n",
    "# Value iteration begins from here.\n",
    "v = np.zeros(num_states)\n",
    "\n",
    "delta = np.inf\n",
    "\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        if s in terminal_states:\n",
    "            continue\n",
    "\n",
    "        old_v = v[s]\n",
    "        new_vs = []\n",
    "        for a in range(len(actions)):\n",
    "            new_v = 0\n",
    "            transitions = transition_dynamics[s][a]\n",
    "            for a_a, p in transitions.items():\n",
    "                # probability * (r + γV(S'))\n",
    "                new_v += (p[0] * (p[2] + gamma * v[p[1]]))\n",
    "            new_vs.append(new_v)\n",
    "        new_v = max(new_vs)\n",
    "        delta = max(delta, abs(new_v - old_v))\n",
    "        v[s] = new_v\n",
    "\n",
    "policy_states = 25\n",
    "policy = ['' for _ in range(policy_states)]\n",
    "\n",
    "# Perform one-step look ahead to derive the policy from the values.\n",
    "for s in range(policy_states):\n",
    "    action_values = np.zeros(len(actions))\n",
    "    for a in range(len(actions)):\n",
    "        for a_a, p in transition_dynamics[s][a].items():\n",
    "            action_values[a] += (p[0] * (p[2]+ v[p[1]]))\n",
    "    policy[s] = action_abrev[np.argmax(action_values)]\n",
    "\n",
    "policy = np.array(policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0caea5678dc94a857977196af8abb31",
     "grade": false,
     "grade_id": "cell-02a5c34a5b828d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Example Test Cell**\n",
    "\n",
    "In the code cell below, we have provided an example of the type of test code that we will use to mark your work. In these tests, we first check that your `policy` and `v` variables are of the correct type, and then check that their values match the solution. For the remaining exercises, the answers will be hidden from you until the model answers are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0721555e5b5cf97791052eab7786e27c",
     "grade": true,
     "grade_id": "cw1_value_iteration_deterministic_tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's policy:\n",
      "[['e' 'e' 'e' 'e' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Solution policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "\n",
      "\n",
      "Student's v:\n",
      "[[ 3.96      6.2       9.        0.        9.      ]\n",
      " [ 2.168     3.96      6.2       0.        6.2     ]\n",
      " [ 0.7344    2.168     3.96      2.168     3.96    ]\n",
      " [-0.41248   0.7344    2.168     0.7344    2.168   ]\n",
      " [-1.329984 -0.41248   0.7344   -0.41248   0.7344  ]]\n",
      "Solution v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 3 decimals\n[Marking Advice] State values incorrect.\nMismatched elements: 21 / 23 (91.3%)\nMax absolute difference: 4.41248\nMax relative difference: 1.443328\n x: array([-1.33 , -0.412,  0.734, -0.412,  0.734, -0.412,  0.734,  2.168,\n        0.734,  2.168,  0.734,  2.168,  3.96 ,  2.168,  3.96 ,  2.168,\n        3.96 ,  6.2  ,  6.2  ,  3.96 ,  6.2  ,  9.   ,  9.   ])\n y: array([3., 4., 5., 4., 5., 4., 5., 6., 5., 6., 5., 6., 7., 6., 7., 6., 7.,\n       8., 8., 7., 8., 9., 9.])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/62/f6c07p1x7qlbltzjcpxsrrsr0000gn/T/ipykernel_63266/1685916471.py\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mstates_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mincorrect_error_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[Marking Advice] State values incorrect.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincorrect_error_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Compare policy (only on states that have a single optimal direction).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    842\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 3 decimals\n[Marking Advice] State values incorrect.\nMismatched elements: 21 / 23 (91.3%)\nMax absolute difference: 4.41248\nMax relative difference: 1.443328\n x: array([-1.33 , -0.412,  0.734, -0.412,  0.734, -0.412,  0.734,  2.168,\n        0.734,  2.168,  0.734,  2.168,  3.96 ,  2.168,  3.96 ,  2.168,\n        3.96 ,  6.2  ,  6.2  ,  3.96 ,  6.2  ,  9.   ,  9.   ])\n y: array([3., 4., 5., 4., 5., 4., 5., 6., 5., 6., 5., 6., 7., 6., 7., 6., 7.,\n       8., 8., 7., 8., 9., 9.])"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 1 is tested here.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We're giving you the solution values for Exercise 1, but not telling you how to compute them!\n",
    "solution_values = [3.0, 4.0, 5.0, 4.0, 5.0,\n",
    "                   4.0, 5.0, 6.0, 5.0, 6.0,\n",
    "                   5.0, 6.0, 7.0, 6.0, 7.0,\n",
    "                   6.0, 7.0, 8.0, 0.0, 8.0,\n",
    "                   7.0, 8.0, 9.0, 0.0, 9.0]\n",
    "solution_values = np.array(solution_values)\n",
    "\n",
    "# We're giving you the solution policy for Exercise 1, but not telling you how to compute it!\n",
    "solution_policy = [\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'e', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'e', 'e', 'e', 'n', 'w',]\n",
    "solution_policy = np.array(solution_policy)\n",
    "\n",
    "# Check that policy and v are numpy arrays.\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.unicode_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether policy contains only \"n\", \"w\", \"s\", or \"e\" values.\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"]))))\n",
    "\n",
    "# Print student's solution and true solution for easier comparison / spotting of errors.\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))\n",
    "print(\"Solution policy:\")\n",
    "print(np.flip(solution_policy.reshape((5, 5)), 0))\n",
    "print(\"\\n\")\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"Solution v:\")\n",
    "print(np.flip(solution_values.reshape((5, 5)), 0))\n",
    "\n",
    "# Compare state values for non-terminal states.\n",
    "states_to_check = np.delete(np.arange(25), np.array([18, 23]))\n",
    "incorrect_error_message = \"[Marking Advice] State values incorrect.\"\n",
    "np.testing.assert_array_almost_equal(v[states_to_check], solution_values[states_to_check], decimal=3, err_msg=incorrect_error_message)\n",
    "\n",
    "# Compare policy (only on states that have a single optimal direction).\n",
    "states_to_check =  np.array([4, 9, 14, 17, 19, 20, 22, 24])\n",
    "incorrect_error_message = \"[Marking Advice] Policy incorrect.\"\n",
    "np.testing.assert_array_equal(policy[states_to_check], solution_policy[states_to_check], err_msg=incorrect_error_message)\n",
    "\n",
    "# Compare state values for terminal states.\n",
    "states_to_check =  np.array([18, 23])\n",
    "incorrect_error_message = \"[Marking Advice] Value of terminal state is not zero.\"\n",
    "np.testing.assert_array_equal(v[states_to_check], solution_values[states_to_check], err_msg=incorrect_error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ed35f6133bda044807637edec8d9fd6",
     "grade": false,
     "grade_id": "cell-24b8f994cc50fe2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Stochastic Environment\n",
    "\n",
    "In this exercise, we introduce stochasticity into the environment. Now, the agent is not always able to execute its actions as intended.\n",
    "\n",
    "With probability 0.8, the agent moves as intended. However, with probability 0.2, it moves in a random direction.\n",
    "\n",
    "For example, from grid square 0, if the agent tries to move north, with probability 0.8 the action will work as intended. But with probability 0.2, the agent's motor control system will move it in a random direction (including north). So, it will randomly try to move west, east, north or south with probability 0.05 each. Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e39b2d999356a5a459fe81749e96e67",
     "grade": false,
     "grade_id": "cw1_value_iteration_stochastic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "num_states = 25\n",
    "\n",
    "gold_states = [23]\n",
    "terminal_states = [18, 23]\n",
    "bomb_states = [18]\n",
    "\n",
    "stochasticity = 0.8\n",
    "\n",
    "theta = 1e-10\n",
    "gamma = 1 # Discount Factor\n",
    "\n",
    "transition_dynamics = create_transition_dynamics(num_states, gold_states, terminal_states, bomb_states, stochasticity)\n",
    "\n",
    "v = np.zeros(num_states)\n",
    "\n",
    "delta = np.inf\n",
    "\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        if s in terminal_states:\n",
    "            continue\n",
    "\n",
    "        old_v = v[s]\n",
    "        new_vs = []\n",
    "        for a in range(len(actions)):\n",
    "            new_v = 0\n",
    "            transitions = transition_dynamics[s][a]\n",
    "            for a_a, p in transitions.items():\n",
    "                # probability * (r + γV(S'))\n",
    "                new_v += (p[0] * (p[2] + gamma * v[p[1]]))\n",
    "            new_vs.append(new_v)\n",
    "        new_v = max(new_vs)\n",
    "        delta = max(delta, abs(new_v - old_v))\n",
    "        v[s] = new_v\n",
    "\n",
    "policy_states = 25\n",
    "policy = ['' for _ in range(policy_states)]\n",
    "\n",
    "for s in range(policy_states):\n",
    "    action_values = np.zeros(len(actions))\n",
    "    for a in range(len(actions)):\n",
    "        for a_a, p in transition_dynamics[s][a].items():\n",
    "            action_values[a] += (p[0] * (p[2]+ v[p[1]]))\n",
    "    policy[s] = action_abrev[np.argmax(action_values)]\n",
    "\n",
    "v = np.around(v, 2)\n",
    "policy = np.array(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f74c2390fd78c29fbaa9041639aab861",
     "grade": true,
     "grade_id": "cw1_value_iteration_stochastic_tests_1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's v:\n",
      "[[6.04 7.29 8.61 0.   8.69]\n",
      " [4.86 5.99 6.37 0.   6.47]\n",
      " [3.68 4.7  4.99 3.22 5.1 ]\n",
      " [2.49 3.41 3.67 2.64 3.79]\n",
      " [1.36 2.2  2.43 1.57 2.55]]\n",
      "\n",
      "\n",
      "Student's policy:\n",
      "[['e' 'e' 'e' 'e' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"\\n\")\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45507ff59702c530cadf05e4a75a0d91",
     "grade": false,
     "grade_id": "cell-e0de56802818cf9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Stochastic Environment with Two Pieces of Gold (6 Marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this exercise, we have modified the environment presented in exercise 2. A second piece of gold has been placed on grid square 12, and a terminal state is reached only when **both** pieces of gold have been collected, or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy for this altered environment using Value Iteration.\n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in the previous exercises. Each value in these arrays should specify the expected return and optimal policy at the corresponding grid sqaure **before any pieces of gold are collected or a bomb is activated.** \n",
    "\n",
    "Hint: You will need to change your state representation in order to account for the additional piece of gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8ba98477adae2eaf9d8c007241dce2a",
     "grade": false,
     "grade_id": "cw1_value_iteration_two_gold",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "\n",
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm.\n",
    "\n",
    "num_states = 75\n",
    "\n",
    "gold_states = [12, 23, 48, 62]\n",
    "terminal_states = [18, 43, 48, 62, 68]\n",
    "bomb_states = [18, 43, 68]\n",
    "\n",
    "stochasticity = 0.8\n",
    "\n",
    "theta = 1e-10\n",
    "gamma = 1 # Discount Factor\n",
    "\n",
    "transition_dynamics = create_transition_dynamics(num_states, gold_states, terminal_states, bomb_states, stochasticity)\n",
    "\n",
    "# print_transition_dynamics(transition_dynamics)\n",
    "\n",
    "v = np.zeros(num_states)\n",
    "\n",
    "delta = np.inf\n",
    "\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        if s in terminal_states:\n",
    "            continue\n",
    "\n",
    "        old_v = v[s]\n",
    "        new_vs = []\n",
    "        for a in range(len(actions)):\n",
    "            new_v = 0\n",
    "            transitions = transition_dynamics[s][a]\n",
    "            for a_a, p in transitions.items():\n",
    "                # probability * (r + γV(S'))\n",
    "                new_v += (p[0] * (p[2] + gamma * v[p[1]]))\n",
    "            new_vs.append(new_v)\n",
    "        new_v = max(new_vs)\n",
    "        delta = max(delta, abs(new_v - old_v))\n",
    "        v[s] = new_v\n",
    "\n",
    "policy_states = 25\n",
    "policy = ['' for _ in range(policy_states)]\n",
    "\n",
    "for s in range(policy_states):\n",
    "    action_values = np.zeros(len(actions))\n",
    "    for a in range(len(actions)):\n",
    "        for a_a, p in transition_dynamics[s][a].items():\n",
    "            action_values[a] += (p[0] * (p[2]+ v[p[1]]))\n",
    "    policy[s] = action_abrev[np.argmax(action_values)]\n",
    "\n",
    "policy = np.array(policy)\n",
    "\n",
    "v = v[:25]\n",
    "v = np.around(v, 2)\n",
    "policy = policy[:25]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43e1fe235001ee4eccea7ab6e93905df",
     "grade": true,
     "grade_id": "cw1_value_iteration_two_gold_tests_1",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's v:\n",
      "[[10.65 11.8  13.01 10.74 12.97]\n",
      " [11.19 12.33 12.51  0.   10.62]\n",
      " [12.29 13.59 12.48 12.42 11.2 ]\n",
      " [11.18 12.35 13.59 12.28 11.05]\n",
      " [10.06 11.17 12.28 11.11  9.99]]\n",
      "\n",
      "\n",
      "Student's policy:\n",
      "[['e' 'e' 'e' 'w' 'w']\n",
      " ['e' 's' 's' 'n' 'n']\n",
      " ['e' 'e' 'w' 'w' 'w']\n",
      " ['e' 'n' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'w']]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"\\n\")\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
